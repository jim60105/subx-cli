# Product Backlog #16.4: 檔案編碼自動檢測實作

## 領域範圍
檔案編碼檢測、字符編碼轉換、文字解析最佳化、多語言支援

## 背景描述

**更新日期**: 2025-06-08  
**架構狀況**: 基於統一配置管理系統 (Backlog #14 已完成)  
**前置條件**: 統一配置管理系統、現有格式引擎架構

隨著 SubX 在全球範圍內的使用增加，處理不同字符編碼的字幕檔案成為重要需求。目前的格式引擎雖然支援多種字幕格式，但缺乏自動編碼檢測功能，導致使用者在處理非 UTF-8 編碼的檔案時經常遇到亂碼問題。

實作檔案編碼自動檢測系統可以顯著提升使用者體驗，自動處理不同地區和語言的字幕檔案，減少手動配置的需求。

## 功能概述

### 檔案編碼自動檢測系統 (Automatic Encoding Detection)
**相關配置**: `formats.default_encoding`, `formats.encoding_detection_confidence`  
**目標模組**: `src/core/formats/encoding/` (新增目錄)

#### 核心功能描述
- **自動編碼檢測**: 智慧識別字幕檔案的字符編碼
- **多編碼支援**: 支援常見的字符編碼格式 (UTF-8, UTF-16, GBK, Shift-JIS 等)
- **編碼轉換**: 自動將檔案轉換為統一的內部編碼格式
- **錯誤修復**: 處理編碼錯誤和損壞的字符
- **統一配置整合**: 透過 `load_config()` 載入編碼處理參數

#### 技術需求與挑戰
- **編碼檢測演算法**: 基於字節模式和統計分析的檢測方法
- **字符編碼轉換**: 高效且準確的編碼轉換處理
- **錯誤處理**: 優雅處理編碼錯誤和不完整的字符序列
- **效能最佳化**: 快速處理大型字幕檔案

## 詳細實作計劃

### 階段 1: 編碼檢測引擎建立 (預估工時: 14 小時)

#### 1.1 建立編碼檢測系統架構
```rust
// src/core/formats/encoding/mod.rs
pub mod detector;
pub mod converter;
pub mod analyzer;
pub mod charset;

pub use detector::EncodingDetector;
pub use converter::{EncodingConverter, ConversionResult};
pub use analyzer::{ByteAnalyzer, StatisticalAnalyzer};
pub use charset::{Charset, EncodingInfo};
```

**架構設計重點**:
- **模組化設計**: 分離檢測、轉換和分析功能
- **可擴展性**: 支援新增更多編碼格式
- **統一介面**: 為格式引擎提供一致的編碼處理介面

#### 1.2 實作編碼檢測器核心
```rust
// src/core/formats/encoding/detector.rs
use std::collections::HashMap;
use std::fs::File;
use std::io::{BufReader, Read};
use crate::config::load_config;
use crate::Result;

#[derive(Debug, Clone, PartialEq)]
pub enum Charset {
    Utf8,
    Utf16Le,
    Utf16Be,
    Utf32Le,
    Utf32Be,
    Gbk,
    ShiftJis,
    Iso88591,
    Windows1252,
    Big5,
    Euckr,
    Unknown,
}

#[derive(Debug, Clone)]
pub struct EncodingInfo {
    pub charset: Charset,
    pub confidence: f32,        // 檢測信心度 (0.0-1.0)
    pub bom_detected: bool,     // 是否檢測到 BOM
    pub sample_text: String,    // 解碼後的樣本文字
}

pub struct EncodingDetector {
    confidence_threshold: f32,
    max_sample_size: usize,
    supported_charsets: Vec<Charset>,
}

impl EncodingDetector {
    pub fn new() -> Result<Self> {
        let config = load_config()?;
        Ok(Self {
            confidence_threshold: config.formats.encoding_detection_confidence,
            max_sample_size: 8192, // 8KB 樣本
            supported_charsets: Self::default_charsets(),
        })
    }

    pub fn detect_file_encoding(&self, file_path: &str) -> Result<EncodingInfo> {
        let mut file = File::open(file_path)?;
        let mut buffer = vec![0; self.max_sample_size];
        let bytes_read = file.read(&mut buffer)?;
        buffer.truncate(bytes_read);
        
        self.detect_encoding(&buffer)
    }

    pub fn detect_encoding(&self, data: &[u8]) -> Result<EncodingInfo> {
        // 1. 檢查 BOM (Byte Order Mark)
        if let Some(encoding) = self.detect_bom(data) {
            return Ok(encoding);
        }

        // 2. 統計分析檢測
        let candidates = self.analyze_byte_patterns(data)?;
        
        // 3. 選擇最佳候選
        self.select_best_encoding(candidates, data)
    }

    fn detect_bom(&self, data: &[u8]) -> Option<EncodingInfo> {
        if data.len() < 3 {
            return None;
        }

        match &data[0..3] {
            [0xEF, 0xBB, 0xBF] => Some(EncodingInfo {
                charset: Charset::Utf8,
                confidence: 1.0,
                bom_detected: true,
                sample_text: String::from("UTF-8 with BOM"),
            }),
            [0xFF, 0xFE, ..] => Some(EncodingInfo {
                charset: Charset::Utf16Le,
                confidence: 1.0,
                bom_detected: true,
                sample_text: String::from("UTF-16 LE with BOM"),
            }),
            [0xFE, 0xFF, ..] => Some(EncodingInfo {
                charset: Charset::Utf16Be,
                confidence: 1.0,
                bom_detected: true,
                sample_text: String::from("UTF-16 BE with BOM"),
            }),
            _ => {
                // 檢查 UTF-32 BOM
                if data.len() >= 4 {
                    match &data[0..4] {
                        [0xFF, 0xFE, 0x00, 0x00] => Some(EncodingInfo {
                            charset: Charset::Utf32Le,
                            confidence: 1.0,
                            bom_detected: true,
                            sample_text: String::from("UTF-32 LE with BOM"),
                        }),
                        [0x00, 0x00, 0xFE, 0xFF] => Some(EncodingInfo {
                            charset: Charset::Utf32Be,
                            confidence: 1.0,
                            bom_detected: true,
                            sample_text: String::from("UTF-32 BE with BOM"),
                        }),
                        _ => None,
                    }
                } else {
                    None
                }
            }
        }
    }

    fn analyze_byte_patterns(&self, data: &[u8]) -> Result<Vec<EncodingCandidate>> {
        let mut candidates = Vec::new();

        for charset in &self.supported_charsets {
            let confidence = self.calculate_encoding_confidence(data, charset)?;
            if confidence > 0.1 { // 只保留有一定信心度的候選
                candidates.push(EncodingCandidate {
                    charset: charset.clone(),
                    confidence,
                });
            }
        }

        // 按信心度排序
        candidates.sort_by(|a, b| b.confidence.partial_cmp(&a.confidence).unwrap());
        Ok(candidates)
    }

    fn calculate_encoding_confidence(&self, data: &[u8], charset: &Charset) -> Result<f32> {
        match charset {
            Charset::Utf8 => self.check_utf8_validity(data),
            Charset::Gbk => self.check_gbk_patterns(data),
            Charset::ShiftJis => self.check_shift_jis_patterns(data),
            Charset::Big5 => self.check_big5_patterns(data),
            Charset::Iso88591 => self.check_iso88591_patterns(data),
            Charset::Windows1252 => self.check_windows1252_patterns(data),
            _ => Ok(0.0),
        }
    }

    fn check_utf8_validity(&self, data: &[u8]) -> Result<f32> {
        let mut valid_chars = 0;
        let mut total_chars = 0;
        let mut i = 0;

        while i < data.len() {
            total_chars += 1;
            
            if data[i] & 0x80 == 0 {
                // ASCII 字符
                valid_chars += 1;
                i += 1;
            } else if data[i] & 0xE0 == 0xC0 {
                // 2 字節 UTF-8
                if i + 1 < data.len() && data[i + 1] & 0xC0 == 0x80 {
                    valid_chars += 1;
                }
                i += 2;
            } else if data[i] & 0xF0 == 0xE0 {
                // 3 字節 UTF-8
                if i + 2 < data.len() 
                    && data[i + 1] & 0xC0 == 0x80 
                    && data[i + 2] & 0xC0 == 0x80 {
                    valid_chars += 1;
                }
                i += 3;
            } else if data[i] & 0xF8 == 0xF0 {
                // 4 字節 UTF-8
                if i + 3 < data.len() 
                    && data[i + 1] & 0xC0 == 0x80 
                    && data[i + 2] & 0xC0 == 0x80 
                    && data[i + 3] & 0xC0 == 0x80 {
                    valid_chars += 1;
                }
                i += 4;
            } else {
                i += 1;
            }
        }

        Ok(if total_chars > 0 {
            valid_chars as f32 / total_chars as f32
        } else {
            0.0
        })
    }

    fn check_gbk_patterns(&self, data: &[u8]) -> Result<f32> {
        let mut valid_chars = 0;
        let mut total_chars = 0;
        let mut i = 0;

        while i < data.len() {
            if data[i] < 0x80 {
                // ASCII 字符
                valid_chars += 1;
                total_chars += 1;
                i += 1;
            } else if i + 1 < data.len() {
                let byte1 = data[i];
                let byte2 = data[i + 1];
                
                // GBK 高字節範圍: 0x81-0xFE, 低字節範圍: 0x40-0xFE (排除 0x7F)
                if (0x81..=0xFE).contains(&byte1) 
                    && ((0x40..=0x7E).contains(&byte2) || (0x80..=0xFE).contains(&byte2)) {
                    valid_chars += 1;
                }
                total_chars += 1;
                i += 2;
            } else {
                total_chars += 1;
                i += 1;
            }
        }

        Ok(if total_chars > 0 {
            valid_chars as f32 / total_chars as f32
        } else {
            0.0
        })
    }

    fn check_shift_jis_patterns(&self, data: &[u8]) -> Result<f32> {
        let mut valid_chars = 0;
        let mut total_chars = 0;
        let mut i = 0;

        while i < data.len() {
            if data[i] < 0x80 {
                // ASCII 字符
                valid_chars += 1;
                total_chars += 1;
                i += 1;
            } else if i + 1 < data.len() {
                let byte1 = data[i];
                let byte2 = data[i + 1];
                
                // Shift-JIS 高字節範圍
                if ((0x81..=0x9F).contains(&byte1) || (0xE0..=0xEF).contains(&byte1))
                    && (0x40..=0xFC).contains(&byte2) && byte2 != 0x7F {
                    valid_chars += 1;
                }
                total_chars += 1;
                i += 2;
            } else {
                total_chars += 1;
                i += 1;
            }
        }

        Ok(if total_chars > 0 {
            valid_chars as f32 / total_chars as f32
        } else {
            0.0
        })
    }

    fn check_big5_patterns(&self, data: &[u8]) -> Result<f32> {
        let mut valid_chars = 0;
        let mut total_chars = 0;
        let mut i = 0;

        while i < data.len() {
            if data[i] < 0x80 {
                // ASCII 字符
                valid_chars += 1;
                total_chars += 1;
                i += 1;
            } else if i + 1 < data.len() {
                let byte1 = data[i];
                let byte2 = data[i + 1];
                
                // Big5 高字節範圍: 0xA1-0xFE, 低字節範圍: 0x40-0x7E, 0xA1-0xFE
                if (0xA1..=0xFE).contains(&byte1) 
                    && ((0x40..=0x7E).contains(&byte2) || (0xA1..=0xFE).contains(&byte2)) {
                    valid_chars += 1;
                }
                total_chars += 1;
                i += 2;
            } else {
                total_chars += 1;
                i += 1;
            }
        }

        Ok(if total_chars > 0 {
            valid_chars as f32 / total_chars as f32
        } else {
            0.0
        })
    }

    fn check_iso88591_patterns(&self, data: &[u8]) -> Result<f32> {
        // ISO-8859-1 所有字節都是有效的，所以需要基於內容特徵判斷
        let ascii_count = data.iter().filter(|&&b| b < 0x80).count();
        let extended_count = data.iter().filter(|&&b| b >= 0x80).count();
        
        // 如果有擴展字符且沒有無效的 UTF-8 序列，可能是 ISO-8859-1
        if extended_count > 0 {
            let utf8_confidence = self.check_utf8_validity(data)?;
            Ok(if utf8_confidence < 0.5 { 0.7 } else { 0.2 })
        } else {
            Ok(0.5) // 純 ASCII 也符合 ISO-8859-1
        }
    }

    fn check_windows1252_patterns(&self, data: &[u8]) -> Result<f32> {
        // Windows-1252 在 0x80-0x9F 範圍有特定字符
        let control_chars = data.iter()
            .filter(|&&b| (0x80..=0x9F).contains(&b))
            .count();
        
        let extended_chars = data.iter()
            .filter(|&&b| b >= 0xA0)
            .count();

        if control_chars > 0 || extended_chars > 0 {
            let utf8_confidence = self.check_utf8_validity(data)?;
            Ok(if utf8_confidence < 0.5 { 0.6 } else { 0.1 })
        } else {
            Ok(0.3) // 可能是 Windows-1252
        }
    }

    fn select_best_encoding(
        &self,
        candidates: Vec<EncodingCandidate>,
        data: &[u8],
    ) -> Result<EncodingInfo> {
        if candidates.is_empty() {
            return Ok(EncodingInfo {
                charset: Charset::Unknown,
                confidence: 0.0,
                bom_detected: false,
                sample_text: String::from("Unable to detect encoding"),
            });
        }

        let best_candidate = &candidates[0];
        
        // 如果信心度低於閾值，使用預設編碼
        if best_candidate.confidence < self.confidence_threshold {
            let config = load_config()?;
            let default_charset = self.parse_charset(&config.formats.default_encoding)?;
            return Ok(EncodingInfo {
                charset: default_charset,
                confidence: 0.5,
                bom_detected: false,
                sample_text: format!("Using default encoding: {}", config.formats.default_encoding),
            });
        }

        // 嘗試解碼樣本文字
        let sample_text = self.decode_sample(data, &best_candidate.charset)?;

        Ok(EncodingInfo {
            charset: best_candidate.charset.clone(),
            confidence: best_candidate.confidence,
            bom_detected: false,
            sample_text,
        })
    }

    fn decode_sample(&self, data: &[u8], charset: &Charset) -> Result<String> {
        let sample_size = std::cmp::min(data.len(), 200); // 取前 200 字節作為樣本
        let sample_data = &data[0..sample_size];

        match charset {
            Charset::Utf8 => {
                String::from_utf8(sample_data.to_vec())
                    .or_else(|_| String::from_utf8_lossy(sample_data).into_owned().into())
            }
            Charset::Gbk => {
                // 這裡需要實作 GBK 解碼，暫時使用 UTF-8 lossy
                Ok(String::from_utf8_lossy(sample_data).into_owned())
            }
            Charset::ShiftJis => {
                // 這裡需要實作 Shift-JIS 解碼，暫時使用 UTF-8 lossy
                Ok(String::from_utf8_lossy(sample_data).into_owned())
            }
            _ => Ok(String::from_utf8_lossy(sample_data).into_owned()),
        }
    }

    fn parse_charset(&self, encoding_name: &str) -> Result<Charset> {
        match encoding_name.to_lowercase().as_str() {
            "utf-8" | "utf8" => Ok(Charset::Utf8),
            "utf-16le" | "utf16le" => Ok(Charset::Utf16Le),
            "utf-16be" | "utf16be" => Ok(Charset::Utf16Be),
            "gbk" => Ok(Charset::Gbk),
            "shift-jis" | "shift_jis" | "sjis" => Ok(Charset::ShiftJis),
            "big5" => Ok(Charset::Big5),
            "iso-8859-1" | "iso88591" => Ok(Charset::Iso88591),
            "windows-1252" | "cp1252" => Ok(Charset::Windows1252),
            _ => Ok(Charset::Utf8), // 預設為 UTF-8
        }
    }

    fn default_charsets() -> Vec<Charset> {
        vec![
            Charset::Utf8,
            Charset::Gbk,
            Charset::ShiftJis,
            Charset::Big5,
            Charset::Iso88591,
            Charset::Windows1252,
        ]
    }
}

#[derive(Debug, Clone)]
struct EncodingCandidate {
    charset: Charset,
    confidence: f32,
}

impl Default for EncodingDetector {
    fn default() -> Self {
        Self::new().unwrap_or_else(|_| Self {
            confidence_threshold: 0.7,
            max_sample_size: 8192,
            supported_charsets: Self::default_charsets(),
        })
    }
}
```

#### 1.3 實作編碼轉換器
```rust
// src/core/formats/encoding/converter.rs
use std::collections::HashMap;
use encoding_rs::{Encoding, UTF_8, GBK, SHIFT_JIS, BIG5, WINDOWS_1252, ISO_8859_2};
use crate::core::formats::encoding::{Charset, EncodingInfo};
use crate::Result;

#[derive(Debug, Clone)]
pub struct ConversionResult {
    pub converted_text: String,
    pub original_encoding: Charset,
    pub target_encoding: Charset,
    pub bytes_processed: usize,
    pub had_errors: bool,
    pub error_count: usize,
}

pub struct EncodingConverter {
    encoding_map: HashMap<Charset, &'static Encoding>,
}

impl EncodingConverter {
    pub fn new() -> Self {
        let mut encoding_map = HashMap::new();
        encoding_map.insert(Charset::Utf8, UTF_8);
        encoding_map.insert(Charset::Gbk, GBK);
        encoding_map.insert(Charset::ShiftJis, SHIFT_JIS);
        encoding_map.insert(Charset::Big5, BIG5);
        encoding_map.insert(Charset::Windows1252, WINDOWS_1252);
        encoding_map.insert(Charset::Iso88591, ISO_8859_2); // 近似替代

        Self { encoding_map }
    }

    pub fn convert_to_utf8(
        &self,
        data: &[u8],
        source_encoding: &Charset,
    ) -> Result<ConversionResult> {
        if *source_encoding == Charset::Utf8 {
            // 已經是 UTF-8，直接返回
            return Ok(ConversionResult {
                converted_text: String::from_utf8_lossy(data).into_owned(),
                original_encoding: Charset::Utf8,
                target_encoding: Charset::Utf8,
                bytes_processed: data.len(),
                had_errors: false,
                error_count: 0,
            });
        }

        let encoding = self.encoding_map.get(source_encoding)
            .ok_or_else(|| crate::SubXError::EncodingError(
                format!("Unsupported encoding: {:?}", source_encoding)
            ))?;

        let (converted_text, _, had_errors) = encoding.decode(data);
        
        // 統計錯誤數量
        let error_count = if had_errors {
            self.count_replacement_chars(&converted_text)
        } else {
            0
        };

        Ok(ConversionResult {
            converted_text: converted_text.into_owned(),
            original_encoding: source_encoding.clone(),
            target_encoding: Charset::Utf8,
            bytes_processed: data.len(),
            had_errors,
            error_count,
        })
    }

    pub fn convert_file_to_utf8(
        &self,
        file_path: &str,
        encoding_info: &EncodingInfo,
    ) -> Result<ConversionResult> {
        use std::fs;
        let data = fs::read(file_path)?;
        
        // 如果檢測到 BOM，跳過 BOM 字節
        let data_without_bom = if encoding_info.bom_detected {
            self.skip_bom(&data, &encoding_info.charset)
        } else {
            data.as_slice()
        };

        self.convert_to_utf8(data_without_bom, &encoding_info.charset)
    }

    fn skip_bom(&self, data: &[u8], charset: &Charset) -> &[u8] {
        match charset {
            Charset::Utf8 if data.len() >= 3 && &data[0..3] == [0xEF, 0xBB, 0xBF] => &data[3..],
            Charset::Utf16Le if data.len() >= 2 && &data[0..2] == [0xFF, 0xFE] => &data[2..],
            Charset::Utf16Be if data.len() >= 2 && &data[0..2] == [0xFE, 0xFF] => &data[2..],
            Charset::Utf32Le if data.len() >= 4 && &data[0..4] == [0xFF, 0xFE, 0x00, 0x00] => &data[4..],
            Charset::Utf32Be if data.len() >= 4 && &data[0..4] == [0x00, 0x00, 0xFE, 0xFF] => &data[4..],
            _ => data,
        }
    }

    fn count_replacement_chars(&self, text: &str) -> usize {
        text.chars().filter(|&c| c == '\u{FFFD}').count()
    }

    pub fn validate_conversion(&self, result: &ConversionResult) -> ValidationResult {
        ValidationResult {
            is_valid: !result.had_errors || result.error_count == 0,
            confidence: if result.had_errors {
                1.0 - (result.error_count as f32 / result.converted_text.len() as f32)
            } else {
                1.0
            },
            warnings: self.generate_warnings(result),
        }
    }

    fn generate_warnings(&self, result: &ConversionResult) -> Vec<String> {
        let mut warnings = Vec::new();

        if result.had_errors {
            warnings.push(format!(
                "Encoding conversion had {} replacement characters",
                result.error_count
            ));
        }

        if result.error_count > result.bytes_processed / 10 {
            warnings.push("High error rate detected - encoding may be incorrect".to_string());
        }

        warnings
    }
}

#[derive(Debug, Clone)]
pub struct ValidationResult {
    pub is_valid: bool,
    pub confidence: f32,
    pub warnings: Vec<String>,
}

impl Default for EncodingConverter {
    fn default() -> Self {
        Self::new()
    }
}
```

### 階段 2: 統計分析器實作 (預估工時: 8 小時)

#### 2.1 實作字節模式分析器
```rust
// src/core/formats/encoding/analyzer.rs
use std::collections::HashMap;
use crate::core::formats::encoding::Charset;
use crate::Result;

pub struct ByteAnalyzer {
    byte_frequency: HashMap<u8, usize>,
    bigram_frequency: HashMap<(u8, u8), usize>,
    total_bytes: usize,
}

impl ByteAnalyzer {
    pub fn new() -> Self {
        Self {
            byte_frequency: HashMap::new(),
            bigram_frequency: HashMap::new(),
            total_bytes: 0,
        }
    }

    pub fn analyze(&mut self, data: &[u8]) -> Result<AnalysisResult> {
        self.collect_statistics(data);
        self.calculate_metrics()
    }

    fn collect_statistics(&mut self, data: &[u8]) {
        self.total_bytes = data.len();

        // 收集單字節頻率
        for &byte in data {
            *self.byte_frequency.entry(byte).or_insert(0) += 1;
        }

        // 收集雙字節頻率
        for window in data.windows(2) {
            if let [b1, b2] = window {
                *self.bigram_frequency.entry((*b1, *b2)).or_insert(0) += 1;
            }
        }
    }

    fn calculate_metrics(&self) -> Result<AnalysisResult> {
        let ascii_ratio = self.calculate_ascii_ratio();
        let entropy = self.calculate_entropy();
        let control_char_ratio = self.calculate_control_char_ratio();

        Ok(AnalysisResult {
            ascii_ratio,
            entropy,
            control_char_ratio,
            byte_distribution: self.byte_frequency.clone(),
            likely_encodings: self.suggest_encodings(ascii_ratio, entropy, control_char_ratio),
        })
    }

    fn calculate_ascii_ratio(&self) -> f32 {
        let ascii_count = self.byte_frequency.iter()
            .filter(|(&byte, _)| byte < 0x80)
            .map(|(_, &count)| count)
            .sum::<usize>();

        if self.total_bytes > 0 {
            ascii_count as f32 / self.total_bytes as f32
        } else {
            0.0
        }
    }

    fn calculate_entropy(&self) -> f32 {
        let mut entropy = 0.0;
        for &count in self.byte_frequency.values() {
            if count > 0 {
                let probability = count as f32 / self.total_bytes as f32;
                entropy -= probability * probability.log2();
            }
        }
        entropy
    }

    fn calculate_control_char_ratio(&self) -> f32 {
        let control_count = self.byte_frequency.iter()
            .filter(|(&byte, _)| byte < 0x20 && byte != 0x09 && byte != 0x0A && byte != 0x0D)
            .map(|(_, &count)| count)
            .sum::<usize>();

        if self.total_bytes > 0 {
            control_count as f32 / self.total_bytes as f32
        } else {
            0.0
        }
    }

    fn suggest_encodings(&self, ascii_ratio: f32, entropy: f32, control_ratio: f32) -> Vec<Charset> {
        let mut suggestions = Vec::new();

        // 基於統計特徵建議編碼
        if ascii_ratio > 0.9 {
            suggestions.push(Charset::Utf8);
        }

        if entropy > 6.0 && ascii_ratio < 0.8 {
            suggestions.extend_from_slice(&[Charset::Gbk, Charset::Big5, Charset::ShiftJis]);
        }

        if control_ratio > 0.01 {
            suggestions.push(Charset::Windows1252);
        }

        if suggestions.is_empty() {
            suggestions.push(Charset::Utf8); // 預設建議
        }

        suggestions
    }
}

#[derive(Debug, Clone)]
pub struct AnalysisResult {
    pub ascii_ratio: f32,
    pub entropy: f32,
    pub control_char_ratio: f32,
    pub byte_distribution: HashMap<u8, usize>,
    pub likely_encodings: Vec<Charset>,
}

pub struct StatisticalAnalyzer {
    language_models: HashMap<Charset, LanguageModel>,
}

impl StatisticalAnalyzer {
    pub fn new() -> Self {
        Self {
            language_models: Self::build_language_models(),
        }
    }

    fn build_language_models() -> HashMap<Charset, LanguageModel> {
        let mut models = HashMap::new();

        // UTF-8 語言模型（基於常見字符頻率）
        models.insert(Charset::Utf8, LanguageModel {
            charset: Charset::Utf8,
            common_patterns: vec![
                // 常見 UTF-8 多字節序列開始字節
                (0xC2, 0.05), (0xC3, 0.08), (0xE2, 0.12),
                (0xE3, 0.15), (0xE4, 0.18), (0xE5, 0.20),
            ],
            invalid_patterns: vec![
                // 無效的 UTF-8 續接字節
                (0x80, 0.0), (0xBF, 0.0),
            ],
        });

        // GBK 語言模型
        models.insert(Charset::Gbk, LanguageModel {
            charset: Charset::Gbk,
            common_patterns: vec![
                // 常見 GBK 高字節
                (0xB0, 0.15), (0xC4, 0.12), (0xD6, 0.10),
                (0xB8, 0.08), (0xBF, 0.06), (0xCE, 0.05),
            ],
            invalid_patterns: vec![
                (0x7F, 0.0), // GBK 不使用 0x7F
            ],
        });

        models
    }

    pub fn analyze_with_models(&self, data: &[u8]) -> Result<HashMap<Charset, f32>> {
        let mut scores = HashMap::new();

        for (charset, model) in &self.language_models {
            let score = self.calculate_model_score(data, model)?;
            scores.insert(charset.clone(), score);
        }

        Ok(scores)
    }

    fn calculate_model_score(&self, data: &[u8], model: &LanguageModel) -> Result<f32> {
        let mut score = 0.0;
        let mut pattern_matches = 0;

        for &byte in data {
            // 檢查常見模式
            for &(pattern_byte, weight) in &model.common_patterns {
                if byte == pattern_byte {
                    score += weight;
                    pattern_matches += 1;
                }
            }

            // 檢查無效模式（降低分數）
            for &(invalid_byte, penalty) in &model.invalid_patterns {
                if byte == invalid_byte {
                    score -= 0.1; // 懲罰無效模式
                }
            }
        }

        // 正規化分數
        Ok(if data.len() > 0 {
            score / data.len() as f32
        } else {
            0.0
        })
    }
}

#[derive(Debug, Clone)]
struct LanguageModel {
    charset: Charset,
    common_patterns: Vec<(u8, f32)>, // (字節, 權重)
    invalid_patterns: Vec<(u8, f32)>, // (字節, 懲罰)
}

impl Default for ByteAnalyzer {
    fn default() -> Self {
        Self::new()
    }
}

impl Default for StatisticalAnalyzer {
    fn default() -> Self {
        Self::new()
    }
}
```

### 階段 3: 格式引擎整合 (預估工時: 6 小時)

#### 3.1 更新格式管理器以支援編碼檢測
```rust
// src/core/formats/mod.rs 中的擴展
use crate::core::formats::encoding::{EncodingDetector, EncodingConverter, EncodingInfo};

impl FormatManager {
    // ...existing code...

    pub fn read_subtitle_with_encoding_detection(&self, file_path: &str) -> Result<String> {
        // 1. 檢測檔案編碼
        let detector = EncodingDetector::new()?;
        let encoding_info = detector.detect_file_encoding(file_path)?;

        // 2. 轉換為 UTF-8
        let converter = EncodingConverter::new();
        let conversion_result = converter.convert_file_to_utf8(file_path, &encoding_info)?;

        // 3. 驗證轉換結果
        let validation = converter.validate_conversion(&conversion_result);
        if !validation.is_valid {
            tracing::warn!(
                "Encoding conversion had issues: {:?}",
                validation.warnings
            );
        }

        // 4. 記錄編碼檢測結果
        tracing::info!(
            "Detected encoding: {:?} (confidence: {:.2})",
            encoding_info.charset,
            encoding_info.confidence
        );

        Ok(conversion_result.converted_text)
    }

    pub fn get_encoding_info(&self, file_path: &str) -> Result<EncodingInfo> {
        let detector = EncodingDetector::new()?;
        detector.detect_file_encoding(file_path)
    }
}
```

#### 3.2 實作編碼檢測 CLI 命令
```rust
// src/commands/detect_encoding_command.rs (新增檔案)
use std::path::Path;
use crate::core::formats::encoding::EncodingDetector;
use crate::cli::ui::print_table;
use crate::Result;

pub fn detect_encoding_command(file_paths: &[String], verbose: bool) -> Result<()> {
    let detector = EncodingDetector::new()?;
    let mut results = Vec::new();

    for file_path in file_paths {
        if !Path::new(file_path).exists() {
            tracing::error!("File not found: {}", file_path);
            continue;
        }

        match detector.detect_file_encoding(file_path) {
            Ok(encoding_info) => {
                results.push((file_path.clone(), encoding_info));
            }
            Err(e) => {
                tracing::error!("Failed to detect encoding for {}: {}", file_path, e);
            }
        }
    }

    // 顯示結果
    display_encoding_results(&results, verbose)?;

    Ok(())
}

fn display_encoding_results(
    results: &[(String, crate::core::formats::encoding::EncodingInfo)],
    verbose: bool,
) -> Result<()> {
    use crate::cli::table::Table;

    let mut table = Table::new();
    table.set_header(vec!["檔案", "編碼", "信心度", "BOM", "樣本文字"]);

    for (file_path, encoding_info) in results {
        let file_name = Path::new(file_path)
            .file_name()
            .and_then(|n| n.to_str())
            .unwrap_or(file_path);

        let confidence_str = format!("{:.1}%", encoding_info.confidence * 100.0);
        let bom_str = if encoding_info.bom_detected { "是" } else { "否" };
        
        let sample_text = if verbose {
            &encoding_info.sample_text
        } else {
            // 截斷樣本文字
            let truncated = if encoding_info.sample_text.len() > 50 {
                format!("{}...", &encoding_info.sample_text[..47])
            } else {
                encoding_info.sample_text.clone()
            };
            &truncated
        };

        table.add_row(vec![
            file_name.to_string(),
            format!("{:?}", encoding_info.charset),
            confidence_str,
            bom_str.to_string(),
            sample_text.to_string(),
        ]);
    }

    print_table(&table);
    Ok(())
}
```

### 階段 4: 測試和驗證 (預估工時: 8 小時)

#### 4.1 單元測試實作
```rust
// src/core/formats/encoding/tests.rs
#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::tempdir;

    #[test]
    fn test_utf8_detection() {
        let detector = EncodingDetector::new().unwrap();
        let utf8_text = "Hello, 世界! 🌍";
        let result = detector.detect_encoding(utf8_text.as_bytes()).unwrap();
        
        assert_eq!(result.charset, Charset::Utf8);
        assert!(result.confidence > 0.8);
    }

    #[test]
    fn test_utf8_with_bom_detection() {
        let detector = EncodingDetector::new().unwrap();
        let mut data = vec![0xEF, 0xBB, 0xBF]; // UTF-8 BOM
        data.extend_from_slice("Hello, World!".as_bytes());
        
        let result = detector.detect_encoding(&data).unwrap();
        
        assert_eq!(result.charset, Charset::Utf8);
        assert_eq!(result.confidence, 1.0);
        assert!(result.bom_detected);
    }

    #[test]
    fn test_gbk_pattern_detection() {
        let detector = EncodingDetector::new().unwrap();
        // 模擬 GBK 編碼的字節序列
        let gbk_data = vec![
            0xC4, 0xE3, 0xBA, 0xC3, // "你好" 的 GBK 編碼
            0xA3, 0xA1, 0xCA, 0xC0, // 更多 GBK 字符
        ];
        
        let result = detector.detect_encoding(&gbk_data).unwrap();
        
        // 應該檢測為 GBK 或者至少不是 UTF-8
        assert_ne!(result.charset, Charset::Utf8);
    }

    #[test]
    fn test_encoding_conversion() {
        let converter = EncodingConverter::new();
        let utf8_text = "測試文字";
        
        let result = converter.convert_to_utf8(
            utf8_text.as_bytes(),
            &Charset::Utf8,
        ).unwrap();
        
        assert_eq!(result.converted_text, utf8_text);
        assert!(!result.had_errors);
        assert_eq!(result.error_count, 0);
    }

    #[test]
    fn test_file_encoding_detection() {
        let detector = EncodingDetector::new().unwrap();
        let temp_dir = tempdir().unwrap();
        let file_path = temp_dir.path().join("test.txt");
        
        // 建立測試檔案
        fs::write(&file_path, "Hello, 世界!").unwrap();
        
        let result = detector.detect_file_encoding(
            file_path.to_str().unwrap()
        ).unwrap();
        
        assert_eq!(result.charset, Charset::Utf8);
    }

    #[test]
    fn test_byte_analyzer() {
        let mut analyzer = ByteAnalyzer::new();
        let test_data = "Hello, World! 123".as_bytes();
        
        let result = analyzer.analyze(test_data).unwrap();
        
        assert!(result.ascii_ratio > 0.9);
        assert!(result.entropy > 0.0);
        assert!(result.control_char_ratio < 0.1);
    }

    #[test]
    fn test_unknown_encoding_fallback() {
        let detector = EncodingDetector::new().unwrap();
        let random_data: Vec<u8> = (0..100).map(|i| (i * 7) as u8).collect();
        
        let result = detector.detect_encoding(&random_data).unwrap();
        
        // 應該回退到預設編碼或標記為未知
        assert!(result.confidence < 0.9);
    }
}
```

#### 4.2 整合測試
```rust
// tests/encoding_integration_tests.rs
use subx_cli::core::formats::encoding::{EncodingDetector, EncodingConverter};
use std::fs;
use tempfile::tempdir;

#[test]
fn test_subtitle_file_encoding_detection() {
    let temp_dir = tempdir().unwrap();
    
    // 建立不同編碼的測試字幕檔案
    let srt_content = r#"1
00:00:01,000 --> 00:00:03,000
Hello, World!

2
00:00:04,000 --> 00:00:06,000
測試字幕
"#;

    let utf8_file = temp_dir.path().join("test_utf8.srt");
    fs::write(&utf8_file, srt_content).unwrap();
    
    let detector = EncodingDetector::new().unwrap();
    let result = detector.detect_file_encoding(
        utf8_file.to_str().unwrap()
    ).unwrap();
    
    assert_eq!(result.charset, subx_cli::core::formats::encoding::Charset::Utf8);
    assert!(result.confidence > 0.7);
}

#[test]
fn test_end_to_end_encoding_conversion() {
    let temp_dir = tempdir().unwrap();
    let input_file = temp_dir.path().join("input.srt");
    
    // 建立包含多語言內容的測試檔案
    let content = "Hello, 世界! Bonjour, monde!";
    fs::write(&input_file, content).unwrap();
    
    // 檢測編碼
    let detector = EncodingDetector::new().unwrap();
    let encoding_info = detector.detect_file_encoding(
        input_file.to_str().unwrap()
    ).unwrap();
    
    // 轉換為 UTF-8
    let converter = EncodingConverter::new();
    let conversion_result = converter.convert_file_to_utf8(
        input_file.to_str().unwrap(),
        &encoding_info,
    ).unwrap();
    
    assert_eq!(conversion_result.converted_text, content);
    assert!(!conversion_result.had_errors);
}
```

## 預期效益

### 功能效益
1. **自動編碼檢測**: 使用者無需手動指定檔案編碼
2. **多語言支援**: 支援全球主要字符編碼格式
3. **錯誤修復**: 自動處理編碼錯誤和損壞字符
4. **效能最佳化**: 快速準確的編碼檢測和轉換

### 技術效益
1. **統一配置整合**: 完全整合到現有配置管理系統
2. **模組化設計**: 易於維護和擴展的架構
3. **全面測試**: 完整的單元測試和整合測試覆蓋
4. **效能監控**: 詳細的檢測和轉換效能指標

### 使用者體驗效益
1. **零配置使用**: 開箱即用的編碼處理
2. **智慧提示**: 清晰的編碼檢測結果和建議
3. **錯誤處理**: 優雅的錯誤處理和恢復機制
4. **進度透明**: 詳細的處理進度和狀態資訊

## 實作優先級

### 高優先級 (必須實作)
- 核心編碼檢測引擎
- UTF-8, GBK, Shift-JIS 支援
- BOM 檢測和處理
- 基礎轉換功能

### 中優先級 (重要功能)
- 統計分析器
- 更多編碼格式支援
- 錯誤修復機制
- CLI 命令整合

### 低優先級 (增強功能)
- 進階語言模型
- 自訂編碼規則
- 批次處理最佳化
- 詳細效能報告

## 風險評估

### 技術風險
- **編碼檢測準確度**: 可能出現誤判，特別是相似編碼
- **效能問題**: 大檔案處理可能較慢
- **記憶體使用**: 統計分析可能消耗較多記憶體

### 緩解策略
- 實作多層檢測機制提高準確度
- 使用串流處理處理大檔案
- 最佳化記憶體使用和快取策略
- 提供手動覆蓋選項作為後備方案

---

**預估總工時**: 36 小時  
**建議實作時程**: 5-6 週  
**前置需求**: Backlog #14 (統一配置管理系統)  
**後續依賴**: 無直接依賴，但會增強所有檔案處理功能的穩定性
