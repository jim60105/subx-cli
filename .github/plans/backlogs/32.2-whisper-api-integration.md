# Backlog 32.2: OpenAI Whisper API 整合

## 概覽

實作 OpenAI Whisper API 的整合功能，作為 SubX 同步指令的雲端語音檢測解決方案。此功能將從音訊檔案中提取第一句字幕前後30秒的音訊片段，使用 Whisper API 進行轉錄，並檢測語音的精確開始時間以計算偏移量。這是 [Backlog 32](./32-redesign-sync-command-architecture.md) 的第二個子任務。

## 目標

1. 建立 OpenAI Whisper API 的 HTTP 客戶端整合
2. 實作音訊片段提取和預處理功能
3. 實作 API 回應解析和時間戳提取
4. 建立錯誤處理和重試機制
5. 實作與新配置結構的整合
6. 提供完整的測試覆蓋

## 技術規格

### API 整合架構

```rust
/// OpenAI Whisper API 客戶端
pub struct WhisperApiClient {
    client: reqwest::Client,
    api_key: String,
    base_url: String,
    config: WhisperConfig,
}

/// Whisper API 請求結構
#[derive(Debug, Serialize)]
struct WhisperRequest {
    model: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    language: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,
    response_format: String, // "verbose_json"
    timestamp_granularities: Vec<String>, // ["word", "segment"]
}

/// Whisper API 回應結構
#[derive(Debug, Deserialize)]
pub struct WhisperResponse {
    pub text: String,
    pub segments: Vec<WhisperSegment>,
    pub words: Option<Vec<WhisperWord>>,
}

#[derive(Debug, Deserialize)]
pub struct WhisperSegment {
    pub start: f64,
    pub end: f64,
    pub text: String,
}

#[derive(Debug, Deserialize)]
pub struct WhisperWord {
    pub word: String,
    pub start: f64,
    pub end: f64,
}
```

### 同步檢測邏輯

```rust
/// Whisper 基礎的同步檢測器
pub struct WhisperSyncDetector {
    client: WhisperApiClient,
    audio_processor: AudioProcessor,
}

impl WhisperSyncDetector {
    /// 檢測字幕與音訊的時間偏移
    pub async fn detect_sync_offset(
        &self,
        audio_path: &Path,
        subtitle: &Subtitle,
    ) -> Result<SyncResult> {
        // 1. 提取第一句字幕的時間資訊
        let first_subtitle = self.get_first_subtitle_entry(subtitle)?;
        
        // 2. 提取對應的音訊片段（前後30秒）
        let audio_segment = self.extract_audio_segment(
            audio_path,
            first_subtitle.start_time,
            self.config.analysis_window_seconds,
        ).await?;
        
        // 3. 調用 Whisper API 進行轉錄
        let transcription = self.client.transcribe(&audio_segment).await?;
        
        // 4. 分析轉錄結果，找到第一個語音片段
        let first_speech_time = self.find_first_speech_segment(&transcription)?;
        
        // 5. 計算偏移量
        let offset = self.calculate_offset(first_subtitle.start_time, first_speech_time);
        
        Ok(SyncResult {
            offset_seconds: offset,
            confidence: self.calculate_confidence(&transcription),
            method_used: SyncMethod::WhisperApi,
            additional_info: Some(json!({
                "transcribed_text": transcription.text,
                "detected_speech_start": first_speech_time,
                "original_subtitle_start": first_subtitle.start_time.as_secs_f64(),
            })),
        })
    }
}
```

## 實作步驟

### 步驟 1: 建立 Whisper API 客戶端

**檔案**: `src/services/whisper/client.rs`

```rust
use reqwest::{Client, multipart::Form};
use serde::{Deserialize, Serialize};
use tokio::fs::File;
use tokio_util::codec::{BytesCodec, FramedRead};
use crate::config::WhisperConfig;
use crate::{Result, error::SubXError};

pub struct WhisperApiClient {
    client: Client,
    api_key: String,
    base_url: String,
    config: WhisperConfig,
}

impl WhisperApiClient {
    pub fn new(api_key: String, base_url: String, config: WhisperConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(Duration::from_secs(config.timeout_seconds as u64))
            .build()
            .map_err(|e| SubXError::http(format!("Failed to create HTTP client: {}", e)))?;

        Ok(Self {
            client,
            api_key,
            base_url,
            config,
        })
    }

    /// 轉錄音訊檔案
    pub async fn transcribe(&self, audio_path: &Path) -> Result<WhisperResponse> {
        let mut retries = 0;
        let mut last_error = None;

        while retries <= self.config.max_retries {
            match self.try_transcribe(audio_path).await {
                Ok(response) => return Ok(response),
                Err(e) => {
                    last_error = Some(e);
                    if retries < self.config.max_retries {
                        tokio::time::sleep(Duration::from_millis(self.config.retry_delay_ms)).await;
                        retries += 1;
                    }
                }
            }
        }

        Err(last_error.unwrap_or_else(|| SubXError::api("Unknown Whisper API error")))
    }

    async fn try_transcribe(&self, audio_path: &Path) -> Result<WhisperResponse> {
        // 準備多部分表單資料
        let file = File::open(audio_path).await
            .map_err(|e| SubXError::io(format!("Failed to open audio file: {}", e)))?;

        let file_stream = FramedRead::new(file, BytesCodec::new());
        let file_body = reqwest::Body::wrap_stream(file_stream);

        let form = Form::new()
            .text("model", self.config.model.clone())
            .text("response_format", "verbose_json")
            .text("timestamp_granularities[]", "word")
            .text("timestamp_granularities[]", "segment")
            .part("file", reqwest::multipart::Part::stream(file_body)
                .file_name(audio_path.file_name()
                    .and_then(|n| n.to_str())
                    .unwrap_or("audio.wav"))
                .mime_str("audio/wav")?);

        let form = if self.config.language != "auto" {
            form.text("language", self.config.language.clone())
        } else {
            form
        };

        let form = if self.config.temperature > 0.0 {
            form.text("temperature", self.config.temperature.to_string())
        } else {
            form
        };

        // 發送 API 請求
        let response = self.client
            .post(&format!("{}/audio/transcriptions", self.base_url))
            .header("Authorization", format!("Bearer {}", self.api_key))
            .multipart(form)
            .send()
            .await
            .map_err(|e| SubXError::http(format!("Whisper API request failed: {}", e)))?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(SubXError::api(format!(
                "Whisper API error {}: {}", status, error_text
            )));
        }

        let whisper_response: WhisperResponse = response.json().await
            .map_err(|e| SubXError::api(format!("Failed to parse Whisper response: {}", e)))?;

        Ok(whisper_response)
    }
}

// Response structures
#[derive(Debug, Deserialize)]
pub struct WhisperResponse {
    pub text: String,
    pub segments: Vec<WhisperSegment>,
    pub words: Option<Vec<WhisperWord>>,
}

#[derive(Debug, Deserialize)]
pub struct WhisperSegment {
    pub start: f64,
    pub end: f64,
    pub text: String,
}

#[derive(Debug, Deserialize)]
pub struct WhisperWord {
    pub word: String,
    pub start: f64,
    pub end: f64,
}
```

### 步驟 2: 實作音訊片段提取器

**檔案**: `src/services/whisper/audio_extractor.rs`

```rust
use std::path::{Path, PathBuf};
use std::time::Duration;
use tempfile::NamedTempFile;
use crate::services::audio::AudioTranscoder;
use crate::{Result, error::SubXError};

pub struct AudioSegmentExtractor {
    transcoder: AudioTranscoder,
}

impl AudioSegmentExtractor {
    pub fn new() -> Result<Self> {
        Ok(Self {
            transcoder: AudioTranscoder::new()?,
        })
    }

    /// 提取音訊片段用於 Whisper API 分析
    pub async fn extract_segment(
        &self,
        audio_path: &Path,
        center_time: Duration,
        window_seconds: u32,
    ) -> Result<PathBuf> {
        // 計算開始和結束時間
        let window_duration = Duration::from_secs(window_seconds as u64);
        let half_window = window_duration / 2;
        
        let start_time = if center_time > half_window {
            center_time - half_window
        } else {
            Duration::from_secs(0)
        };
        
        let end_time = center_time + half_window;

        // 建立臨時檔案用於輸出
        let temp_file = NamedTempFile::with_suffix(".wav")
            .map_err(|e| SubXError::io(format!("Failed to create temp file: {}", e)))?;
        let output_path = temp_file.path().to_owned();

        // 使用音訊轉碼器提取片段
        self.transcoder.extract_segment(
            audio_path,
            &output_path,
            start_time,
            end_time,
        ).await?;

        // 返回臨時檔案路徑（呼叫者負責清理）
        Ok(output_path)
    }

    /// 準備音訊檔案以符合 Whisper API 要求
    pub async fn prepare_for_whisper(&self, audio_path: &Path) -> Result<PathBuf> {
        // Whisper API 建議使用的音訊格式：
        // - 採樣率: 16kHz
        // - 格式: WAV
        // - 聲道: 單聲道
        
        let temp_file = NamedTempFile::with_suffix(".wav")
            .map_err(|e| SubXError::io(format!("Failed to create temp file: {}", e)))?;
        let output_path = temp_file.path().to_owned();

        self.transcoder.transcode_to_format(
            audio_path,
            &output_path,
            16000, // 16kHz sample rate
            1,     // mono
        ).await?;

        Ok(output_path)
    }
}
```

### 步驟 3: 實作 Whisper 同步檢測器

**檔案**: `src/services/whisper/sync_detector.rs`

```rust
use std::path::Path;
use std::time::Duration;
use serde_json::json;
use crate::core::formats::{Subtitle, SubtitleEntry};
use crate::core::sync::{SyncResult, SyncMethod};
use crate::config::WhisperConfig;
use crate::{Result, error::SubXError};
use super::{WhisperApiClient, WhisperResponse, AudioSegmentExtractor};

pub struct WhisperSyncDetector {
    client: WhisperApiClient,
    extractor: AudioSegmentExtractor,
    config: WhisperConfig,
}

impl WhisperSyncDetector {
    pub fn new(api_key: String, base_url: String, config: WhisperConfig) -> Result<Self> {
        Ok(Self {
            client: WhisperApiClient::new(api_key, base_url, config.clone())?,
            extractor: AudioSegmentExtractor::new()?,
            config,
        })
    }

    pub async fn detect_sync_offset(
        &self,
        audio_path: &Path,
        subtitle: &Subtitle,
        analysis_window_seconds: u32,
    ) -> Result<SyncResult> {
        // 1. 獲取第一句字幕
        let first_entry = self.get_first_subtitle_entry(subtitle)?;
        
        // 2. 提取音訊片段
        let audio_segment_path = self.extractor.extract_segment(
            audio_path,
            first_entry.start_time,
            analysis_window_seconds,
        ).await?;

        // 3. 準備音訊檔案格式
        let prepared_audio = self.extractor.prepare_for_whisper(&audio_segment_path).await?;

        // 4. 調用 Whisper API
        let transcription = self.client.transcribe(&prepared_audio).await?;

        // 5. 分析轉錄結果
        let analysis_result = self.analyze_transcription(&transcription, &first_entry)?;

        // 6. 清理臨時檔案
        let _ = tokio::fs::remove_file(&audio_segment_path).await;
        let _ = tokio::fs::remove_file(&prepared_audio).await;

        Ok(analysis_result)
    }

    fn get_first_subtitle_entry(&self, subtitle: &Subtitle) -> Result<&SubtitleEntry> {
        subtitle.entries.first()
            .ok_or_else(|| SubXError::sync("No subtitle entries found"))
    }

    fn analyze_transcription(
        &self,
        transcription: &WhisperResponse,
        first_entry: &SubtitleEntry,
    ) -> Result<SyncResult> {
        // 檢測第一個有意義的語音段
        let first_speech_time = self.find_first_speech_segment(transcription)?;
        
        // 計算偏移量
        // 由於我們提取的音訊片段是以第一句字幕為中心的30秒片段
        // 所以需要計算相對於片段開始的實際時間
        let analysis_window = Duration::from_secs(self.config.timeout_seconds as u64);
        let half_window = analysis_window / 2;
        
        let expected_position_in_segment = half_window.as_secs_f64();
        let actual_position_in_segment = first_speech_time;
        
        let offset_seconds = actual_position_in_segment - expected_position_in_segment;
        
        // 計算信心度
        let confidence = self.calculate_confidence(transcription, first_entry);

        Ok(SyncResult {
            offset_seconds: offset_seconds as f32,
            confidence,
            method_used: SyncMethod::WhisperApi,
            additional_info: Some(json!({
                "transcribed_text": transcription.text.trim(),
                "detected_speech_start": first_speech_time,
                "expected_speech_start": expected_position_in_segment,
                "subtitle_text": first_entry.text,
                "segments_count": transcription.segments.len(),
                "words_count": transcription.words.as_ref().map(|w| w.len()).unwrap_or(0),
            })),
        })
    }

    fn find_first_speech_segment(&self, transcription: &WhisperResponse) -> Result<f64> {
        // 優先使用 word-level 時間戳，因為更精確
        if let Some(words) = &transcription.words {
            if let Some(first_word) = words.first() {
                return Ok(first_word.start);
            }
        }

        // 回退到 segment-level 時間戳
        if let Some(first_segment) = transcription.segments.first() {
            return Ok(first_segment.start);
        }

        Err(SubXError::sync("No speech segments found in transcription"))
    }

    fn calculate_confidence(&self, transcription: &WhisperResponse, first_entry: &SubtitleEntry) -> f32 {
        // 基礎信心度從轉錄品質開始
        let mut confidence = 0.8; // 基礎 Whisper API 信心度

        // 如果找到了語音片段，增加信心度
        if !transcription.segments.is_empty() {
            confidence += 0.1;
        }

        // 如果有詳細的 word-level 時間戳，增加信心度
        if transcription.words.is_some() {
            confidence += 0.05;
        }

        // 檢查轉錄文字和字幕文字的相似度
        let similarity = self.calculate_text_similarity(
            &transcription.text,
            &first_entry.text,
        );
        
        confidence += similarity * 0.05; // 最多增加 5% 信心度

        confidence.min(1.0) // 確保不超過 100%
    }

    fn calculate_text_similarity(&self, transcribed: &str, subtitle: &str) -> f32 {
        // 簡單的文字相似度計算
        // 在實際應用中可以使用更複雜的演算法，如 Levenshtein 距離
        let transcribed = transcribed.to_lowercase();
        let subtitle = subtitle.to_lowercase();
        
        let transcribed_words: Vec<&str> = transcribed.split_whitespace().collect();
        let subtitle_words: Vec<&str> = subtitle.split_whitespace().collect();
        
        if transcribed_words.is_empty() || subtitle_words.is_empty() {
            return 0.0;
        }

        let mut common_words = 0;
        for word in &subtitle_words {
            if transcribed_words.contains(word) {
                common_words += 1;
            }
        }

        common_words as f32 / subtitle_words.len() as f32
    }
}
```

### 步驟 4: 建立模組結構

**檔案**: `src/services/whisper/mod.rs`

```rust
//! OpenAI Whisper API 整合模組
//!
//! 此模組提供與 OpenAI Whisper API 的整合功能，用於高精度的語音轉錄
//! 和時間戳檢測，以實現精確的字幕同步。

mod client;
mod audio_extractor;
mod sync_detector;

pub use client::{WhisperApiClient, WhisperResponse, WhisperSegment, WhisperWord};
pub use audio_extractor::AudioSegmentExtractor;
pub use sync_detector::WhisperSyncDetector;

// Re-export for convenience
pub use crate::config::WhisperConfig;
```

### 步驟 5: 更新主要服務模組

**檔案**: `src/services/mod.rs`

```rust
// 現有的模組...
pub mod audio;
pub mod openai;

// 新增 Whisper 服務模組
pub mod whisper;
```

### 步驟 6: 整合到配置服務

**檔案**: `src/core/services.rs` (如果存在) 或建立新的整合點

```rust
use crate::config::{Config, ConfigService};
use crate::services::whisper::WhisperSyncDetector;
use crate::{Result, error::SubXError};

pub struct SyncServiceFactory {
    config_service: Box<dyn ConfigService>,
}

impl SyncServiceFactory {
    pub fn new(config_service: Box<dyn ConfigService>) -> Self {
        Self { config_service }
    }

    pub fn create_whisper_detector(&self) -> Result<WhisperSyncDetector> {
        let config = self.config_service.get_config()?;
        
        // 檢查 Whisper 是否已啟用
        if !config.sync.whisper.enabled {
            return Err(SubXError::config("Whisper API is not enabled in configuration"));
        }

        // 獲取 API 金鑰
        let api_key = config.ai.api_key
            .or_else(|| std::env::var("OPENAI_API_KEY").ok())
            .ok_or_else(|| SubXError::config("OpenAI API key not found"))?;

        WhisperSyncDetector::new(
            api_key,
            config.ai.base_url,
            config.sync.whisper,
        )
    }
}
```

## 測試策略

### 單元測試

**檔案**: `src/services/whisper/client.rs`

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::WhisperConfig;
    use tokio_test;

    #[tokio::test]
    async fn test_whisper_client_creation() {
        let config = WhisperConfig::default();
        let client = WhisperApiClient::new(
            "test-key".to_string(),
            "https://api.openai.com/v1".to_string(),
            config,
        );
        assert!(client.is_ok());
    }

    // 注意：實際 API 測試需要真實的 API 金鑰，因此使用 #[ignore] 標記
    #[tokio::test]
    #[ignore]
    async fn test_whisper_api_transcription() {
        // 此測試需要實際的 API 金鑰和測試音訊檔案
        // 在 CI/CD 環境中可以使用環境變數提供
    }
}
```

### 整合測試

**檔案**: `tests/whisper_integration_tests.rs`

```rust
use subx_cli::services::whisper::{WhisperSyncDetector, AudioSegmentExtractor};
use subx_cli::core::formats::Subtitle;
use subx_cli::config::TestConfigBuilder;
use tempfile::TempDir;
use std::fs;

#[tokio::test]
async fn test_audio_segment_extraction() {
    let temp_dir = TempDir::new().unwrap();
    
    // 建立測試音訊檔案（簡化版）
    let audio_path = temp_dir.path().join("test.wav");
    create_test_audio_file(&audio_path).await;

    let extractor = AudioSegmentExtractor::new().unwrap();
    let segment_path = extractor.extract_segment(
        &audio_path,
        std::time::Duration::from_secs(10),
        30,
    ).await.unwrap();

    assert!(segment_path.exists());
}

#[tokio::test]
#[ignore] // 需要 API 金鑰
async fn test_whisper_sync_detection_integration() {
    // 此測試需要：
    // 1. 真實的 OpenAI API 金鑰
    // 2. 測試音訊檔案
    // 3. 對應的字幕檔案
    
    let config = TestConfigBuilder::new()
        .with_whisper_enabled(true)
        .build_config();
    
    // 實際測試邏輯...
}

async fn create_test_audio_file(path: &std::path::Path) {
    // 建立簡單的測試音訊檔案
    // 實際實作中可使用音訊生成函式庫
    fs::write(path, b"RIFF....WAVE").unwrap();
}
```

### Mock 測試

**檔案**: `tests/whisper_mock_tests.rs`

```rust
use wiremock::{MockServer, Mock, ResponseTemplate};
use wiremock::matchers::{method, path, header};
use serde_json::json;

#[tokio::test]
async fn test_whisper_api_mock_success() {
    // 建立 mock 伺服器
    let mock_server = MockServer::start().await;
    
    // 設定 mock 回應
    Mock::given(method("POST"))
        .and(path("/audio/transcriptions"))
        .and(header("authorization", "Bearer test-key"))
        .respond_with(ResponseTemplate::new(200)
            .set_body_json(json!({
                "text": "Hello world",
                "segments": [{
                    "start": 0.5,
                    "end": 2.0,
                    "text": "Hello world"
                }],
                "words": [{
                    "word": "Hello",
                    "start": 0.5,
                    "end": 1.0
                }, {
                    "word": "world",
                    "start": 1.0,
                    "end": 1.5
                }]
            })))
        .mount(&mock_server)
        .await;

    // 測試 Whisper 客戶端
    let config = WhisperConfig::default();
    let client = WhisperApiClient::new(
        "test-key".to_string(),
        mock_server.uri(),
        config,
    ).unwrap();

    // 測試轉錄功能（需要建立測試音訊檔案）
    // ...
}
```

## 依賴項目

### Cargo.toml 新增依賴

```toml
[dependencies]
# HTTP 客戶端
reqwest = { version = "0.11", features = ["json", "multipart", "stream"] }

# JSON 處理
serde_json = "1.0"

# 異步檔案處理  
tokio-util = { version = "0.7", features = ["codec"] }

# 臨時檔案
tempfile = "3.0"

[dev-dependencies]
# Mock HTTP 伺服器用於測試
wiremock = "0.5"

# 異步測試工具
tokio-test = "0.4"
```

## 錯誤處理

### 新增錯誤類型

**檔案**: `src/error.rs`

```rust
impl SubXError {
    pub fn whisper_api<T: Into<String>>(message: T) -> Self {
        Self::Api {
            message: message.into(),
            source: ApiErrorSource::Whisper,
        }
    }

    pub fn audio_extraction<T: Into<String>>(message: T) -> Self {
        Self::AudioProcessing {
            message: message.into(),
        }
    }
}

#[derive(Debug)]
pub enum ApiErrorSource {
    OpenAI,
    Whisper,
    // 其他 API 來源...
}
```

## 完成標準

1. ✅ WhisperApiClient 正確實作且能與 OpenAI API 通訊
2. ✅ AudioSegmentExtractor 能正確提取和預處理音訊片段
3. ✅ WhisperSyncDetector 能準確檢測同步偏移量
4. ✅ 錯誤處理和重試機制正常運作
5. ✅ 與新配置結構完全整合
6. ✅ 所有單元測試和整合測試通過
7. ✅ Mock 測試覆蓋所有主要的 API 交互場景
8. ✅ 臨時檔案清理機制正常運作
9. ✅ API 金鑰和配置驗證正確實作

## 使用範例

```rust
use subx_cli::services::whisper::WhisperSyncDetector;
use subx_cli::config::TestConfigBuilder;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 建立配置
    let config = TestConfigBuilder::new()
        .with_whisper_enabled(true)
        .with_whisper_model("whisper-1")
        .build_config();

    // 建立檢測器
    let detector = WhisperSyncDetector::new(
        "your-api-key".to_string(),
        "https://api.openai.com/v1".to_string(),
        config.sync.whisper,
    )?;

    // 載入字幕
    let subtitle = load_subtitle("subtitle.srt")?;

    // 檢測同步偏移
    let result = detector.detect_sync_offset(
        Path::new("video.mp4"),
        &subtitle,
        30, // 30 秒分析窗口
    ).await?;

    println!("Detected offset: {:.2}s", result.offset_seconds);
    println!("Confidence: {:.2}%", result.confidence * 100.0);

    Ok(())
}
```

---

**預估工時**: 12 小時  
**依賴項目**: Backlog 32.1 (新配置結構)  
**後續項目**: Backlog 32.3 (本地 VAD 實作)
