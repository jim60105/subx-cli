# Backlog 32.3: 本地 VAD 實作

## 概覽

整合 `voice_activity_detector` crate 實作本地語音活動檢測 (Voice Activity Detection, VAD) 功能，作為 SubX 同步指令的本地語音檢測解決方案。此功能將在本地分析音訊檔案中第一句字幕前後30秒的音訊片段，檢測語音活動的精確開始時間以計算時間偏移量。這是 [Backlog 32](./32-redesign-sync-command-architecture.md) 的第三個子任務。

## 目標

1. 整合 `voice_activity_detector` crate 到專案中
2. 實作本地音訊處理和 VAD 分析管道
3. 實作語音檢測結果分析和時間戳提取
4. 建立音訊格式轉換和預處理功能
5. 實作與新配置結構的整合
6. 提供完整的測試覆蓋和性能最佳化

## 技術規格

### VAD 整合架構

```rust
/// 本地語音活動檢測器
pub struct LocalVadDetector {
    vad: VoiceActivityDetector,
    config: VadConfig,
    audio_processor: VadAudioProcessor,
}

/// VAD 音訊處理器
pub struct VadAudioProcessor {
    target_sample_rate: u32,
    target_channels: u16,
}

/// VAD 檢測結果
#[derive(Debug, Clone)]
pub struct VadResult {
    /// 語音片段清單
    pub speech_segments: Vec<SpeechSegment>,
    /// 總處理時間
    pub processing_duration: Duration,
    /// 音訊檔案資訊
    pub audio_info: AudioInfo,
}

#[derive(Debug, Clone)]
pub struct SpeechSegment {
    /// 開始時間（秒）
    pub start_time: f64,
    /// 結束時間（秒）  
    pub end_time: f64,
    /// 語音機率 (0.0-1.0)
    pub probability: f32,
    /// 持續時間（秒）
    pub duration: f64,
}

#[derive(Debug, Clone)]
pub struct AudioInfo {
    pub sample_rate: u32,
    pub channels: u16,
    pub duration_seconds: f64,
    pub total_samples: usize,
}
```

### 同步檢測邏輯

```rust
/// 基於本地 VAD 的同步檢測器
pub struct VadSyncDetector {
    vad_detector: LocalVadDetector,
    audio_extractor: AudioSegmentExtractor, // 重用自 Whisper 模組
}

impl VadSyncDetector {
    /// 檢測字幕與音訊的時間偏移
    pub async fn detect_sync_offset(
        &self,
        audio_path: &Path,
        subtitle: &Subtitle,
        analysis_window_seconds: u32,
    ) -> Result<SyncResult> {
        // 1. 提取第一句字幕的時間資訊
        let first_subtitle = self.get_first_subtitle_entry(subtitle)?;
        
        // 2. 提取對應的音訊片段（前後30秒）
        let audio_segment = self.extract_audio_segment(
            audio_path,
            first_subtitle.start_time,
            analysis_window_seconds,
        ).await?;
        
        // 3. 使用 VAD 檢測語音片段
        let vad_result = self.vad_detector.detect_speech(&audio_segment).await?;
        
        // 4. 分析檢測結果，找到第一個語音片段
        let first_speech_time = self.find_first_significant_speech(&vad_result)?;
        
        // 5. 計算偏移量
        let offset = self.calculate_offset(
            first_subtitle.start_time,
            first_speech_time,
            analysis_window_seconds,
        );
        
        Ok(SyncResult {
            offset_seconds: offset,
            confidence: self.calculate_confidence(&vad_result),
            method_used: SyncMethod::LocalVad,
            additional_info: Some(json!({
                "speech_segments_count": vad_result.speech_segments.len(),
                "first_speech_start": first_speech_time,
                "processing_time_ms": vad_result.processing_duration.as_millis(),
                "audio_info": vad_result.audio_info,
            })),
        })
    }
}
```

## 實作步驟

### 步驟 1: 新增 VAD 依賴項目

**檔案**: `Cargo.toml`

```toml
[dependencies]
# 語音活動檢測
voice_activity_detector = { version = "0.2.0", features = ["async"] }

# 音訊處理支援
hound = "3.5"           # WAV 檔案讀取
rubato = "0.14"         # 採樣率轉換
```

### 步驟 2: 實作本地 VAD 檢測器

**檔案**: `src/services/vad/detector.rs`

```rust
use voice_activity_detector::{VoiceActivityDetector, IteratorExt, LabeledAudio};
use std::path::Path;
use std::time::{Duration, Instant};
use crate::config::VadConfig;
use crate::{Result, error::SubXError};

pub struct LocalVadDetector {
    config: VadConfig,
    audio_processor: VadAudioProcessor,
}

impl LocalVadDetector {
    pub fn new(config: VadConfig) -> Result<Self> {
        Ok(Self {
            config,
            audio_processor: VadAudioProcessor::new(config.sample_rate, 1)?,
        })
    }

    /// 檢測音訊檔案中的語音活動
    pub async fn detect_speech(&self, audio_path: &Path) -> Result<VadResult> {
        let start_time = Instant::now();

        // 1. 載入和預處理音訊
        let audio_data = self.audio_processor.load_and_prepare_audio(audio_path).await?;
        
        // 2. 建立 VAD 實例
        let mut vad = VoiceActivityDetector::builder()
            .sample_rate(self.config.sample_rate)
            .chunk_size(self.config.chunk_size)
            .build()
            .map_err(|e| SubXError::audio_processing(format!("Failed to create VAD: {}", e)))?;

        // 3. 執行語音檢測
        let speech_segments = self.detect_speech_segments(&mut vad, &audio_data.samples)?;

        let processing_duration = start_time.elapsed();

        Ok(VadResult {
            speech_segments,
            processing_duration,
            audio_info: audio_data.info,
        })
    }

    fn detect_speech_segments(
        &self,
        vad: &mut VoiceActivityDetector,
        samples: &[i16],
    ) -> Result<Vec<SpeechSegment>> {
        let mut segments = Vec::new();
        let chunk_duration_seconds = self.config.chunk_size as f64 / self.config.sample_rate as f64;

        // 使用 label 功能來標識語音和非語音片段
        let labels: Vec<LabeledAudio<i16>> = samples
            .iter()
            .copied()
            .label(vad, self.config.sensitivity, self.config.padding_chunks);

        let mut current_speech_start: Option<f64> = None;
        let mut chunk_index = 0;

        for label in labels {
            let chunk_start_time = chunk_index as f64 * chunk_duration_seconds;
            
            match label {
                LabeledAudio::Speech(_chunk) => {
                    if current_speech_start.is_none() {
                        current_speech_start = Some(chunk_start_time);
                    }
                },
                LabeledAudio::NonSpeech(_chunk) => {
                    if let Some(start_time) = current_speech_start.take() {
                        let end_time = chunk_start_time;
                        let duration = end_time - start_time;
                        
                        // 過濾太短的語音片段
                        if duration >= self.config.min_speech_duration_ms as f64 / 1000.0 {
                            segments.push(SpeechSegment {
                                start_time,
                                end_time,
                                probability: self.config.sensitivity, // 使用配置的敏感度作為機率
                                duration,
                            });
                        }
                    }
                },
            }
            
            chunk_index += 1;
        }

        // 處理最後一個語音片段（如果存在）
        if let Some(start_time) = current_speech_start {
            let end_time = chunk_index as f64 * chunk_duration_seconds;
            let duration = end_time - start_time;
            
            if duration >= self.config.min_speech_duration_ms as f64 / 1000.0 {
                segments.push(SpeechSegment {
                    start_time,
                    end_time,
                    probability: self.config.sensitivity,
                    duration,
                });
            }
        }

        // 合併相近的語音片段
        Ok(self.merge_close_segments(segments))
    }

    fn merge_close_segments(&self, segments: Vec<SpeechSegment>) -> Vec<SpeechSegment> {
        if segments.is_empty() {
            return segments;
        }

        let mut merged = Vec::new();
        let mut current = segments[0].clone();
        let merge_threshold = self.config.speech_merge_gap_ms as f64 / 1000.0;

        for segment in segments.into_iter().skip(1) {
            if segment.start_time - current.end_time <= merge_threshold {
                // 合併片段
                current.end_time = segment.end_time;
                current.duration = current.end_time - current.start_time;
                current.probability = current.probability.max(segment.probability);
            } else {
                // 儲存當前片段，開始新片段
                merged.push(current);
                current = segment;
            }
        }
        
        merged.push(current);
        merged
    }
}

#[derive(Debug, Clone)]
pub struct VadResult {
    pub speech_segments: Vec<SpeechSegment>,
    pub processing_duration: Duration,
    pub audio_info: AudioInfo,
}

#[derive(Debug, Clone)]
pub struct SpeechSegment {
    pub start_time: f64,
    pub end_time: f64,
    pub probability: f32,
    pub duration: f64,
}

#[derive(Debug, Clone)]
pub struct AudioInfo {
    pub sample_rate: u32,
    pub channels: u16,
    pub duration_seconds: f64,
    pub total_samples: usize,
}
```

### 步驟 3: 實作 VAD 音訊處理器

**檔案**: `src/services/vad/audio_processor.rs`

```rust
use hound::{WavReader, SampleFormat};
use rubato::{SincFixedIn, SincInterpolationParameters, SincInterpolationType, Resampler};
use std::path::Path;
use std::fs::File;
use std::io::BufReader;
use crate::{Result, error::SubXError};
use super::{AudioInfo};

pub struct VadAudioProcessor {
    target_sample_rate: u32,
    target_channels: u16,
}

#[derive(Debug)]
pub struct ProcessedAudioData {
    pub samples: Vec<i16>,
    pub info: AudioInfo,
}

impl VadAudioProcessor {
    pub fn new(target_sample_rate: u32, target_channels: u16) -> Result<Self> {
        Ok(Self {
            target_sample_rate,
            target_channels,
        })
    }

    pub async fn load_and_prepare_audio(&self, audio_path: &Path) -> Result<ProcessedAudioData> {
        // 1. 載入音訊檔案
        let raw_audio_data = self.load_wav_file(audio_path)?;
        
        // 2. 轉換採樣率（如果需要）
        let resampled_data = if raw_audio_data.sample_rate != self.target_sample_rate {
            self.resample_audio(&raw_audio_data)?
        } else {
            raw_audio_data
        };

        // 3. 轉換為單聲道（如果需要）
        let mono_data = if resampled_data.channels > 1 {
            self.convert_to_mono(&resampled_data)?
        } else {
            resampled_data
        };

        Ok(mono_data)
    }

    fn load_wav_file(&self, path: &Path) -> Result<ProcessedAudioData> {
        let file = File::open(path)
            .map_err(|e| SubXError::io(format!("Failed to open audio file: {}", e)))?;
        
        let reader = WavReader::new(BufReader::new(file))
            .map_err(|e| SubXError::audio_processing(format!("Failed to read WAV file: {}", e)))?;

        let spec = reader.spec();
        let sample_rate = spec.sample_rate;
        let channels = spec.channels;
        
        // 讀取所有樣本並轉換為 i16
        let samples: Result<Vec<i16>> = match spec.sample_format {
            SampleFormat::Int => {
                match spec.bits_per_sample {
                    16 => {
                        reader.into_samples::<i16>()
                            .collect::<Result<Vec<_>, _>>()
                            .map_err(|e| SubXError::audio_processing(format!("Failed to read samples: {}", e)))
                    },
                    32 => {
                        // 轉換 i32 到 i16
                        let i32_samples: Vec<i32> = reader.into_samples::<i32>()
                            .collect::<Result<Vec<_>, _>>()
                            .map_err(|e| SubXError::audio_processing(format!("Failed to read i32 samples: {}", e)))?;
                        Ok(i32_samples.iter().map(|&s| (s >> 16) as i16).collect())
                    },
                    _ => Err(SubXError::audio_processing(format!("Unsupported bit depth: {}", spec.bits_per_sample)))
                }
            },
            SampleFormat::Float => {
                // 轉換 f32 到 i16
                let f32_samples: Vec<f32> = reader.into_samples::<f32>()
                    .collect::<Result<Vec<_>, _>>()
                    .map_err(|e| SubXError::audio_processing(format!("Failed to read f32 samples: {}", e)))?;
                Ok(f32_samples.iter().map(|&s| (s * 32767.0) as i16).collect())
            },
        }?;

        let duration_seconds = samples.len() as f64 / (sample_rate as f64 * channels as f64);

        Ok(ProcessedAudioData {
            samples,
            info: AudioInfo {
                sample_rate,
                channels,
                duration_seconds,
                total_samples: samples.len(),
            },
        })
    }

    fn resample_audio(&self, audio_data: &ProcessedAudioData) -> Result<ProcessedAudioData> {
        if audio_data.info.sample_rate == self.target_sample_rate {
            return Ok(audio_data.clone());
        }

        // 設定重取樣參數
        let params = SincInterpolationParameters {
            sinc_len: 256,
            f_cutoff: 0.95,
            interpolation: SincInterpolationType::Linear,
            oversampling_factor: 128,
            window: rubato::WindowFunction::BlackmanHarris2,
        };

        // 建立重取樣器
        let mut resampler = SincFixedIn::<f64>::new(
            self.target_sample_rate as f64 / audio_data.info.sample_rate as f64,
            2.0, // max_resample_ratio_relative
            params,
            audio_data.samples.len(),
            audio_data.info.channels as usize,
        ).map_err(|e| SubXError::audio_processing(format!("Failed to create resampler: {}", e)))?;

        // 轉換樣本格式為 f64
        let input_channels = if audio_data.info.channels == 1 {
            vec![audio_data.samples.iter().map(|&s| s as f64 / 32768.0).collect()]
        } else {
            // 處理多聲道音訊
            let mut channels = vec![Vec::new(); audio_data.info.channels as usize];
            for (i, &sample) in audio_data.samples.iter().enumerate() {
                channels[i % audio_data.info.channels as usize].push(sample as f64 / 32768.0);
            }
            channels
        };

        // 執行重取樣
        let output_channels = resampler.process(&input_channels, None)
            .map_err(|e| SubXError::audio_processing(format!("Resampling failed: {}", e)))?;

        // 轉換回 i16 格式
        let mut resampled_samples = Vec::new();
        if audio_data.info.channels == 1 {
            resampled_samples = output_channels[0]
                .iter()
                .map(|&s| (s * 32767.0) as i16)
                .collect();
        } else {
            // 交錯多聲道樣本
            let max_len = output_channels.iter().map(|ch| ch.len()).max().unwrap_or(0);
            for i in 0..max_len {
                for ch in &output_channels {
                    if i < ch.len() {
                        resampled_samples.push((ch[i] * 32767.0) as i16);
                    }
                }
            }
        }

        let duration_seconds = resampled_samples.len() as f64 
            / (self.target_sample_rate as f64 * audio_data.info.channels as f64);

        Ok(ProcessedAudioData {
            samples: resampled_samples,
            info: AudioInfo {
                sample_rate: self.target_sample_rate,
                channels: audio_data.info.channels,
                duration_seconds,
                total_samples: resampled_samples.len(),
            },
        })
    }

    fn convert_to_mono(&self, audio_data: &ProcessedAudioData) -> Result<ProcessedAudioData> {
        if audio_data.info.channels == 1 {
            return Ok(audio_data.clone());
        }

        let channels = audio_data.info.channels as usize;
        let mut mono_samples = Vec::new();

        // 轉換為單聲道（平均所有聲道）
        for chunk in audio_data.samples.chunks_exact(channels) {
            let sum: i32 = chunk.iter().map(|&s| s as i32).sum();
            let average = (sum / channels as i32) as i16;
            mono_samples.push(average);
        }

        let duration_seconds = mono_samples.len() as f64 / audio_data.info.sample_rate as f64;

        Ok(ProcessedAudioData {
            samples: mono_samples,
            info: AudioInfo {
                sample_rate: audio_data.info.sample_rate,
                channels: 1,
                duration_seconds,
                total_samples: mono_samples.len(),
            },
        })
    }
}
```

### 步驟 4: 實作 VAD 同步檢測器

**檔案**: `src/services/vad/sync_detector.rs`

```rust
use std::path::Path;
use std::time::Duration;
use serde_json::json;
use crate::core::formats::{Subtitle, SubtitleEntry};
use crate::core::sync::{SyncResult, SyncMethod};
use crate::config::VadConfig;
use crate::services::whisper::AudioSegmentExtractor; // 重用音訊提取器
use crate::{Result, error::SubXError};
use super::{LocalVadDetector, VadResult, SpeechSegment};

pub struct VadSyncDetector {
    vad_detector: LocalVadDetector,
    audio_extractor: AudioSegmentExtractor,
}

impl VadSyncDetector {
    pub fn new(config: VadConfig) -> Result<Self> {
        Ok(Self {
            vad_detector: LocalVadDetector::new(config)?,
            audio_extractor: AudioSegmentExtractor::new()?,
        })
    }

    pub async fn detect_sync_offset(
        &self,
        audio_path: &Path,
        subtitle: &Subtitle,
        analysis_window_seconds: u32,
    ) -> Result<SyncResult> {
        // 1. 獲取第一句字幕
        let first_entry = self.get_first_subtitle_entry(subtitle)?;
        
        // 2. 提取音訊片段
        let audio_segment_path = self.audio_extractor.extract_segment(
            audio_path,
            first_entry.start_time,
            analysis_window_seconds,
        ).await?;

        // 3. 使用 VAD 檢測語音活動
        let vad_result = self.vad_detector.detect_speech(&audio_segment_path).await?;

        // 4. 分析檢測結果
        let analysis_result = self.analyze_vad_result(
            &vad_result,
            first_entry,
            analysis_window_seconds,
        )?;

        // 5. 清理臨時檔案
        let _ = tokio::fs::remove_file(&audio_segment_path).await;

        Ok(analysis_result)
    }

    fn get_first_subtitle_entry(&self, subtitle: &Subtitle) -> Result<&SubtitleEntry> {
        subtitle.entries.first()
            .ok_or_else(|| SubXError::sync("No subtitle entries found"))
    }

    fn analyze_vad_result(
        &self,
        vad_result: &VadResult,
        first_entry: &SubtitleEntry,
        analysis_window_seconds: u32,
    ) -> Result<SyncResult> {
        // 檢測第一個顯著的語音片段  
        let first_speech_time = self.find_first_significant_speech(vad_result)?;
        
        // 計算偏移量
        // 由於我們提取的音訊片段是以第一句字幕為中心的指定秒數片段
        // 所以需要計算相對於片段開始的實際時間
        let half_window = analysis_window_seconds as f64 / 2.0;
        let expected_position_in_segment = half_window;
        let actual_position_in_segment = first_speech_time;
        
        let offset_seconds = actual_position_in_segment - expected_position_in_segment;
        
        // 計算信心度
        let confidence = self.calculate_confidence(vad_result);

        Ok(SyncResult {
            offset_seconds: offset_seconds as f32,
            confidence,
            method_used: SyncMethod::LocalVad,
            additional_info: Some(json!({
                "speech_segments_count": vad_result.speech_segments.len(),
                "first_speech_start": first_speech_time,
                "expected_speech_start": expected_position_in_segment,
                "processing_time_ms": vad_result.processing_duration.as_millis(),
                "audio_duration": vad_result.audio_info.duration_seconds,
                "detected_segments": vad_result.speech_segments.iter().map(|s| {
                    json!({
                        "start": s.start_time,
                        "end": s.end_time,
                        "duration": s.duration,
                        "probability": s.probability
                    })
                }).collect::<Vec<_>>(),
            })),
        })
    }

    fn find_first_significant_speech(&self, vad_result: &VadResult) -> Result<f64> {
        // 尋找第一個顯著的語音片段
        for segment in &vad_result.speech_segments {
            // 檢查片段是否足夠長且機率足夠高
            if segment.duration >= 0.1 && segment.probability >= 0.5 {
                return Ok(segment.start_time);
            }
        }

        // 如果沒找到顯著的語音片段，但有語音片段，回傳第一個
        if let Some(first_segment) = vad_result.speech_segments.first() {
            return Ok(first_segment.start_time);
        }

        Err(SubXError::sync("No significant speech segments found in audio"))
    }

    fn calculate_confidence(&self, vad_result: &VadResult) -> f32 {
        if vad_result.speech_segments.is_empty() {
            return 0.0;
        }

        let mut confidence = 0.6; // 基礎本地 VAD 信心度

        // 基於語音片段數量調整信心度
        let segments_count = vad_result.speech_segments.len();
        if segments_count >= 1 {
            confidence += 0.1;
        }
        if segments_count >= 3 {
            confidence += 0.1;
        }

        // 基於第一個語音片段的品質調整信心度
        if let Some(first_segment) = vad_result.speech_segments.first() {
            // 較長的語音片段增加信心度
            if first_segment.duration >= 0.5 {
                confidence += 0.1;
            }
            if first_segment.duration >= 1.0 {
                confidence += 0.05;
            }

            // 較高的機率增加信心度
            if first_segment.probability >= 0.8 {
                confidence += 0.05;
            }
        }

        // 基於處理速度調整信心度（本地處理通常很快）
        if vad_result.processing_duration.as_secs() <= 1 {
            confidence += 0.05;
        }

        confidence.min(0.95) // 本地 VAD 最高信心度限制為 95%
    }
}
```

### 步驟 5: 建立 VAD 模組結構

**檔案**: `src/services/vad/mod.rs`

```rust
//! 本地語音活動檢測 (Voice Activity Detection) 模組
//!
//! 此模組提供基於 `voice_activity_detector` crate 的本地語音檢測功能，
//! 用於在本地環境中進行快速、私密的語音活動檢測和字幕同步。

mod detector;
mod audio_processor;
mod sync_detector;

pub use detector::{LocalVadDetector, VadResult, SpeechSegment, AudioInfo};
pub use audio_processor::{VadAudioProcessor, ProcessedAudioData};
pub use sync_detector::VadSyncDetector;

// Re-export for convenience
pub use crate::config::VadConfig;
```

### 步驟 6: 更新服務模組

**檔案**: `src/services/mod.rs`

```rust
// 現有的模組...
pub mod audio;
pub mod openai;
pub mod whisper;

// 新增 VAD 服務模組
pub mod vad;
```

## 測試策略

### 單元測試

**檔案**: `src/services/vad/detector.rs`

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::VadConfig;
    use tempfile::TempDir;
    use hound::WavWriter;
    use std::fs::File;

    #[tokio::test]
    async fn test_vad_detector_creation() {
        let config = VadConfig::default();
        let detector = LocalVadDetector::new(config);
        assert!(detector.is_ok());
    }

    #[tokio::test]
    async fn test_speech_detection_with_simple_audio() {
        let temp_dir = TempDir::new().unwrap();
        let audio_path = temp_dir.path().join("test_speech.wav");
        
        // 建立包含語音模式的測試音訊
        create_test_audio_with_speech_pattern(&audio_path);
        
        let config = VadConfig::default();
        let detector = LocalVadDetector::new(config).unwrap();
        
        let result = detector.detect_speech(&audio_path).await.unwrap();
        
        // 驗證檢測到語音片段
        assert!(!result.speech_segments.is_empty());
        assert!(result.processing_duration.as_millis() > 0);
    }

    #[test]
    fn test_segment_merging() {
        let config = VadConfig {
            speech_merge_gap_ms: 500,
            ..Default::default()
        };
        let detector = LocalVadDetector::new(config).unwrap();

        let segments = vec![
            SpeechSegment {
                start_time: 1.0,
                end_time: 2.0,
                probability: 0.8,
                duration: 1.0,
            },
            SpeechSegment {
                start_time: 2.3, // 300ms gap - should merge
                end_time: 3.5,
                probability: 0.9,
                duration: 1.2,
            },
            SpeechSegment {
                start_time: 5.0, // 1.5s gap - should not merge
                end_time: 6.0,
                probability: 0.7,
                duration: 1.0,
            },
        ];

        let merged = detector.merge_close_segments(segments);
        
        // 前兩個片段應該被合併
        assert_eq!(merged.len(), 2);
        assert_eq!(merged[0].start_time, 1.0);
        assert_eq!(merged[0].end_time, 3.5);
        assert_eq!(merged[1].start_time, 5.0);
    }

    fn create_test_audio_with_speech_pattern(path: &std::path::Path) {
        let spec = hound::WavSpec {
            channels: 1,
            sample_rate: 16000,
            bits_per_sample: 16,
            sample_format: hound::SampleFormat::Int,
        };

        let mut writer = WavWriter::create(path, spec).unwrap();

        // 建立包含語音模式的音訊：靜音 -> 語音 -> 靜音 -> 語音
        let total_samples = 16000 * 4; // 4 秒
        
        for i in 0..total_samples {
            let t = i as f32 / 16000.0;
            let sample = if (t >= 1.0 && t <= 2.0) || (t >= 3.0 && t <= 3.5) {
                // 語音片段：生成複合波形模擬語音
                (0.3 * (2.0 * std::f32::consts::PI * 440.0 * t).sin() + 
                 0.2 * (2.0 * std::f32::consts::PI * 880.0 * t).sin()) * 32767.0
            } else {
                // 靜音片段：少量雜音
                ((t * 12345.0).sin() * 0.01) * 32767.0
            };
            
            writer.write_sample(sample as i16).unwrap();
        }
        
        writer.finalize().unwrap();
    }
}
```

### 整合測試

**檔案**: `tests/vad_integration_tests.rs`

```rust
use subx_cli::services::vad::{VadSyncDetector, LocalVadDetector};
use subx_cli::core::formats::{Subtitle, SubtitleEntry, SubtitleMetadata, SubtitleFormatType};
use subx_cli::config::{TestConfigBuilder, VadConfig};
use tempfile::TempDir;
use std::time::Duration;

#[tokio::test]
async fn test_vad_sync_detection_integration() {
    let temp_dir = TempDir::new().unwrap();
    
    // 建立測試音訊檔案
    let audio_path = temp_dir.path().join("test_video.wav");
    create_test_audio_with_timed_speech(&audio_path);
    
    // 建立測試字幕
    let subtitle = create_test_subtitle_with_known_timing();
    
    // 建立 VAD 同步檢測器
    let config = VadConfig::default();
    let detector = VadSyncDetector::new(config).unwrap();
    
    // 執行同步檢測
    let result = detector.detect_sync_offset(
        &audio_path,
        &subtitle,
        30, // 30 秒分析窗口
    ).await.unwrap();
    
    // 驗證結果
    assert!(result.confidence > 0.5);
    assert_eq!(result.method_used, subx_cli::core::sync::SyncMethod::LocalVad);
    assert!(result.additional_info.is_some());
}

#[tokio::test]
async fn test_vad_audio_format_compatibility() {
    let temp_dir = TempDir::new().unwrap();
    
    // 測試不同的音訊格式和參數
    let test_cases = vec![
        (8000, 1),   // 8kHz mono
        (16000, 1),  // 16kHz mono  
        (44100, 1),  // 44.1kHz mono
        (44100, 2),  // 44.1kHz stereo
    ];

    let config = VadConfig::default();
    let detector = LocalVadDetector::new(config).unwrap();

    for (sample_rate, channels) in test_cases {
        let audio_path = temp_dir.path().join(&format!("test_{}_{}.wav", sample_rate, channels));
        create_test_audio_with_format(&audio_path, sample_rate, channels);
        
        let result = detector.detect_speech(&audio_path).await;
        assert!(result.is_ok(), "Failed for format: {}Hz, {} channels", sample_rate, channels);
    }
}

fn create_test_audio_with_timed_speech(path: &std::path::Path) {
    // 建立包含已知時間點語音的測試音訊
    let spec = hound::WavSpec {
        channels: 1,
        sample_rate: 16000,
        bits_per_sample: 16,
        sample_format: hound::SampleFormat::Int,
    };

    let mut writer = hound::WavWriter::create(path, spec).unwrap();
    let duration_seconds = 60; // 1 分鐘
    let total_samples = 16000 * duration_seconds;

    for i in 0..total_samples {
        let t = i as f32 / 16000.0;
        
        // 在第 30 秒附近（分析窗口中心）建立語音
        let sample = if t >= 29.5 && t <= 32.0 {
            // 語音信號
            (0.4 * (2.0 * std::f32::consts::PI * 300.0 * t).sin() + 
             0.3 * (2.0 * std::f32::consts::PI * 600.0 * t).sin()) * 32767.0
        } else {
            // 背景雜音
            ((t * 7919.0).sin() * 0.005) * 32767.0
        };
        
        writer.write_sample(sample as i16).unwrap();
    }
    
    writer.finalize().unwrap();
}

fn create_test_subtitle_with_known_timing() -> Subtitle {
    Subtitle {
        entries: vec![
            SubtitleEntry::new(
                1,
                Duration::from_secs(30), // 第一句在第 30 秒
                Duration::from_secs(32),
                "Test dialogue".to_string(),
            ),
        ],
        metadata: SubtitleMetadata::default(),
        format: SubtitleFormatType::Srt,
    }
}

fn create_test_audio_with_format(path: &std::path::Path, sample_rate: u32, channels: u16) {
    let spec = hound::WavSpec {
        channels,
        sample_rate,
        bits_per_sample: 16,
        sample_format: hound::SampleFormat::Int,
    };

    let mut writer = hound::WavWriter::create(path, spec).unwrap();
    let duration_seconds = 2;
    let total_samples = sample_rate * duration_seconds;

    for i in 0..total_samples {
        let t = i as f32 / sample_rate as f32;
        
        for _ch in 0..channels {
            let sample = if t >= 0.5 && t <= 1.5 {
                // 語音信號
                (0.3 * (2.0 * std::f32::consts::PI * 440.0 * t).sin()) * 32767.0
            } else {
                // 靜音
                0.0
            };
            
            writer.write_sample(sample as i16).unwrap();
        }
    }
    
    writer.finalize().unwrap();
}
```

### 性能測試

**檔案**: `tests/vad_performance_tests.rs`

```rust
use subx_cli::services::vad::LocalVadDetector;
use subx_cli::config::VadConfig;
use std::time::Instant;
use tempfile::TempDir;

#[tokio::test]
#[ignore] // 標記為性能測試，平時不執行
async fn test_vad_performance_large_audio() {
    let temp_dir = TempDir::new().unwrap();
    let audio_path = temp_dir.path().join("large_audio.wav");
    
    // 建立 10 分鐘的測試音訊
    create_long_audio_file(&audio_path, 600);
    
    let config = VadConfig::default();
    let detector = LocalVadDetector::new(config).unwrap();
    
    let start_time = Instant::now();
    let result = detector.detect_speech(&audio_path).await.unwrap();
    let processing_time = start_time.elapsed();
    
    // 驗證性能要求
    assert!(processing_time.as_secs() < 30, "Processing took too long: {:?}", processing_time);
    assert!(!result.speech_segments.is_empty());
    
    println!("Processed 10 minutes of audio in {:?}", processing_time);
    println!("Found {} speech segments", result.speech_segments.len());
}

fn create_long_audio_file(path: &std::path::Path, duration_seconds: u32) {
    let spec = hound::WavSpec {
        channels: 1,
        sample_rate: 16000,
        bits_per_sample: 16,
        sample_format: hound::SampleFormat::Int,
    };

    let mut writer = hound::WavWriter::create(path, spec).unwrap();
    let total_samples = 16000 * duration_seconds;

    for i in 0..total_samples {
        let t = i as f32 / 16000.0;
        
        // 每 10 秒建立一個語音片段
        let is_speech_time = (t % 10.0) >= 2.0 && (t % 10.0) <= 5.0;
        
        let sample = if is_speech_time {
            // 語音信號
            (0.3 * (2.0 * std::f32::consts::PI * 400.0 * t).sin() + 
             0.2 * (2.0 * std::f32::consts::PI * 800.0 * t).sin()) * 32767.0
        } else {
            // 背景雜音
            ((t * 9973.0).sin() * 0.01) * 32767.0
        };
        
        if i % (16000 * 30) == 0 {
            println!("Generated {} seconds of audio", i / 16000);
        }
        
        writer.write_sample(sample as i16).unwrap();
    }
    
    writer.finalize().unwrap();
}
```

## 完成標準

1. ✅ `voice_activity_detector` crate 正確整合到專案中
2. ✅ LocalVadDetector 能正確檢測語音活動
3. ✅ VadAudioProcessor 能處理多種音訊格式並正確轉換
4. ✅ VadSyncDetector 能準確檢測同步偏移量
5. ✅ 語音片段合併和過濾邏輯正常運作
6. ✅ 與新配置結構完全整合
7. ✅ 所有單元測試和整合測試通過
8. ✅ 性能測試證實能在合理時間內處理大型音訊檔案
9. ✅ 支援多種音訊格式和採樣率
10. ✅ 錯誤處理機制完整且健全

## 使用範例

```rust
use subx_cli::services::vad::{VadSyncDetector, LocalVadDetector};
use subx_cli::config::{TestConfigBuilder, VadConfig};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 建立配置
    let config = VadConfig {
        enabled: true,
        sensitivity: 0.75,
        chunk_size: 512,
        sample_rate: 16000,
        padding_chunks: 3,
        min_speech_duration_ms: 100,
        speech_merge_gap_ms: 200,
    };

    // 建立檢測器
    let detector = VadSyncDetector::new(config)?;

    // 載入字幕
    let subtitle = load_subtitle("subtitle.srt")?;

    // 檢測同步偏移
    let result = detector.detect_sync_offset(
        Path::new("video.mp4"),
        &subtitle,
        30, // 30 秒分析窗口
    ).await?;

    println!("Detected offset: {:.2}s", result.offset_seconds);
    println!("Confidence: {:.2}%", result.confidence * 100.0);
    println!("Processing method: Local VAD");

    // 也可以直接使用 VAD 檢測器進行語音檢測
    let vad_detector = LocalVadDetector::new(config)?;
    let vad_result = vad_detector.detect_speech(Path::new("audio.wav")).await?;
    
    println!("Found {} speech segments", vad_result.speech_segments.len());
    for (i, segment) in vad_result.speech_segments.iter().enumerate() {
        println!("Segment {}: {:.2}s - {:.2}s (probability: {:.2})", 
            i + 1, segment.start_time, segment.end_time, segment.probability);
    }

    Ok(())
}
```

---

**預估工時**: 10 小時  
**依賴項目**: Backlog 32.1 (新配置結構), Backlog 32.2 (共享音訊提取器)  
**後續項目**: Backlog 32.4 (同步引擎重構)
