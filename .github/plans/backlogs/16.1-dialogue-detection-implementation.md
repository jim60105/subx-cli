# Product Backlog #16.1: 對話檢測功能實作

## 領域範圍
音訊對話檢測、語音活動檢測、智慧同步時間點識別

## 背景描述

**更新日期**: 2025-06-08  
**架構狀況**: 基於統一配置管理系統 (Backlog #14 已完成)  
**前置條件**: 統一配置管理系統、基礎音訊同步引擎

根據統一配置系統實作完成後的分析，發現 SubX 的音訊同步功能需要更精確的對話檢測能力。當前同步引擎主要依賴音訊波形相關性分析，但缺乏對實際語音內容的智慧識別，導致在複雜音訊環境中同步準確度不佳。

## 功能概述

### 對話檢測功能 (Dialogue Detection)
**相關配置**: `sync.dialogue_detection_threshold`, `sync.min_dialogue_duration_ms`  
**目標模組**: `src/core/sync/dialogue.rs` (新增)

#### 核心功能描述
- **自動檢測音訊中的對話片段**: 識別語音活動期間和靜默間隙
- **語音活動檢測 (VAD)**: 區分語音與背景音樂、噪音等非語音內容
- **對話片段分割**: 將連續音訊流分割為有意義的對話單元
- **時間點最佳化**: 為現有同步引擎提供更精確的同步參考點
- **統一配置整合**: 透過 `load_config()` 載入檢測參數

#### 技術需求與挑戰
- **音訊能量分析**: 基於音訊能量變化檢測語音活動
- **頻譜分析**: 識別人聲特有的頻率特徵
- **時間域處理**: 滑動視窗分析和閾值判斷
- **噪音抑制**: 過濾背景音樂和環境噪音干擾

## 詳細實作計劃

### 階段 1: 音訊分析基礎架構 (預估工時: 12 小時)

#### 1.1 建立對話檢測模組結構
```rust
// src/core/sync/dialogue.rs
pub mod detector;
pub mod analyzer;
pub mod segment;

pub use detector::DialogueDetector;
pub use analyzer::{AudioAnalyzer, EnergyAnalyzer};
pub use segment::{DialogueSegment, SilenceSegment};
```

**實作重點**:
- 建立清晰的模組架構
- 定義統一的公開介面
- 確保與現有同步引擎的相容性

#### 1.2 實作音訊能量分析器
```rust
// src/core/sync/dialogue/analyzer.rs
use std::collections::VecDeque;
use crate::config::load_config;

pub struct EnergyAnalyzer {
    window_size: usize,           // 分析視窗大小
    hop_size: usize,              // 滑動步長
    threshold: f32,               // 語音檢測閾值
    min_duration_ms: u64,         // 最小對話持續時間
}

impl EnergyAnalyzer {
    pub fn new(threshold: f32, min_duration_ms: u64) -> Self {
        Self {
            window_size: 1024,
            hop_size: 512,
            threshold,
            min_duration_ms,
        }
    }
    
    pub fn analyze(&self, audio_data: &[f32], sample_rate: u32) -> Vec<DialogueSegment> {
        let mut segments = Vec::new();
        let mut energy_buffer = VecDeque::new();
        
        // 滑動視窗能量計算
        for (i, chunk) in audio_data.chunks(self.hop_size).enumerate() {
            let energy = self.calculate_energy(chunk);
            energy_buffer.push_back(energy);
            
            if energy_buffer.len() > self.window_size / self.hop_size {
                energy_buffer.pop_front();
            }
            
            // 檢測語音活動
            let is_speech = self.detect_speech(&energy_buffer);
            let timestamp = (i * self.hop_size) as f64 / sample_rate as f64;
            
            // 建立對話片段
            if is_speech {
                if let Some(last_segment) = segments.last_mut() {
                    if last_segment.is_speech {
                        last_segment.end_time = timestamp;
                    } else {
                        segments.push(DialogueSegment::new_speech(timestamp, timestamp));
                    }
                } else {
                    segments.push(DialogueSegment::new_speech(timestamp, timestamp));
                }
            }
        }
        
        // 過濾過短的片段
        self.filter_short_segments(segments)
    }
}
```

**技術細節**:
- **滑動視窗處理**: 使用固定大小視窗分析音訊能量變化
- **動態閾值調整**: 根據音訊特性自動調整檢測閾值
- **時間軸對齊**: 確保檢測結果與音訊時間軸精確對應

#### 1.3 定義對話片段資料結構
```rust
// src/core/sync/dialogue/segment.rs
#[derive(Debug, Clone)]
pub struct DialogueSegment {
    pub start_time: f64,          // 開始時間（秒）
    pub end_time: f64,            // 結束時間（秒）
    pub is_speech: bool,          // 是否為語音片段
    pub confidence: f32,          // 檢測信心度 (0.0-1.0)
}

impl DialogueSegment {
    pub fn new_speech(start: f64, end: f64) -> Self {
        Self {
            start_time: start,
            end_time: end,
            is_speech: true,
            confidence: 1.0,
        }
    }
    
    pub fn new_silence(start: f64, end: f64) -> Self {
        Self {
            start_time: start,
            end_time: end,
            is_speech: false,
            confidence: 1.0,
        }
    }
    
    pub fn duration(&self) -> f64 {
        self.end_time - self.start_time
    }
    
    pub fn overlaps_with(&self, other: &DialogueSegment) -> bool {
        self.start_time < other.end_time && self.end_time > other.start_time
    }
    
    pub fn merge_with(&mut self, other: &DialogueSegment) {
        if self.overlaps_with(other) && self.is_speech == other.is_speech {
            self.start_time = self.start_time.min(other.start_time);
            self.end_time = self.end_time.max(other.end_time);
            self.confidence = (self.confidence + other.confidence) / 2.0;
        }
    }
}

#[derive(Debug, Clone)]
pub struct SilenceSegment {
    pub start_time: f64,
    pub end_time: f64,
    pub duration: f64,
}

impl SilenceSegment {
    pub fn new(start: f64, end: f64) -> Self {
        Self {
            start_time: start,
            end_time: end,
            duration: end - start,
        }
    }
    
    pub fn is_significant(&self, min_duration: f64) -> bool {
        self.duration >= min_duration
    }
}
```

### 階段 2: 對話檢測器整合 (預估工時: 8 小時)

#### 2.1 實作統一對話檢測器
```rust
// src/core/sync/dialogue/detector.rs
use crate::config::{load_config, SyncConfig};
use crate::core::sync::dialogue::{EnergyAnalyzer, DialogueSegment};
use crate::services::audio::AudioData;
use crate::Result;
use std::path::Path;

pub struct DialogueDetector {
    energy_analyzer: EnergyAnalyzer,
    config: SyncConfig,
}

impl DialogueDetector {
    pub fn new() -> Result<Self> {
        let config = load_config()?.sync;
        let energy_analyzer = EnergyAnalyzer::new(
            config.dialogue_detection_threshold,
            config.min_dialogue_duration_ms,
        );
        
        Ok(Self {
            energy_analyzer,
            config,
        })
    }
    
    pub async fn detect_dialogue(&self, audio_path: &Path) -> Result<Vec<DialogueSegment>> {
        // 載入音訊檔案
        let audio_data = self.load_audio(audio_path).await?;
        
        // 執行對話檢測
        let segments = self.energy_analyzer.analyze(&audio_data.samples, audio_data.sample_rate);
        
        // 後處理和最佳化
        let optimized_segments = self.optimize_segments(segments);
        
        Ok(optimized_segments)
    }
    
    async fn load_audio(&self, audio_path: &Path) -> Result<AudioData> {
        // 使用現有的 AudioAnalyzer
        use crate::services::audio::AudioAnalyzer;
        let analyzer = AudioAnalyzer::new()?;
        analyzer.load_audio_file(audio_path).await
    }
    
    fn optimize_segments(&self, segments: Vec<DialogueSegment>) -> Vec<DialogueSegment> {
        // 合併相鄰的語音片段
        let mut optimized = Vec::new();
        let mut current_segment: Option<DialogueSegment> = None;
        
        for segment in segments {
            match current_segment.take() {
                Some(mut prev) if prev.is_speech && segment.is_speech => {
                    // 檢查間隔是否足夠小以進行合併
                    if segment.start_time - prev.end_time < 0.5 { // 0.5 秒間隔
                        prev.end_time = segment.end_time;
                        current_segment = Some(prev);
                    } else {
                        optimized.push(prev);
                        current_segment = Some(segment);
                    }
                }
                Some(prev) => {
                    optimized.push(prev);
                    current_segment = Some(segment);
                }
                None => {
                    current_segment = Some(segment);
                }
            }
        }
        
        if let Some(segment) = current_segment {
            optimized.push(segment);
        }
        
        optimized
    }
    
    pub fn get_speech_ratio(&self, segments: &[DialogueSegment]) -> f32 {
        let total_duration: f64 = segments.iter().map(|s| s.duration()).sum();
        let speech_duration: f64 = segments.iter()
            .filter(|s| s.is_speech)
            .map(|s| s.duration())
            .sum();
        
        if total_duration > 0.0 {
            (speech_duration / total_duration) as f32
        } else {
            0.0
        }
    }
}
```

#### 2.2 整合到現有同步引擎
```rust
// 修改 src/core/sync/engine.rs
use crate::core::sync::dialogue::DialogueDetector;

impl SyncEngine {
    pub async fn sync_with_dialogue_detection(
        &self,
        subtitle_path: &Path,
        audio_path: &Path,
    ) -> Result<Vec<SyncResult>> {
        // 檢測對話片段
        let dialogue_detector = DialogueDetector::new()?;
        let dialogue_segments = dialogue_detector.detect_dialogue(audio_path).await?;
        
        println!("檢測到 {} 個對話片段", dialogue_segments.len());
        println!("語音比例: {:.1}%", 
            dialogue_detector.get_speech_ratio(&dialogue_segments) * 100.0);
        
        // 載入字幕
        let subtitles = self.load_subtitles(subtitle_path).await?;
        
        // 使用對話片段輔助同步
        let sync_results = self.sync_with_dialogue_hints(&subtitles, &dialogue_segments).await?;
        
        Ok(sync_results)
    }
    
    async fn sync_with_dialogue_hints(
        &self,
        subtitles: &[Subtitle],
        dialogue_segments: &[DialogueSegment],
    ) -> Result<Vec<SyncResult>> {
        let mut results = Vec::new();
        
        for subtitle in subtitles {
            // 尋找最匹配的對話片段
            let best_match = self.find_best_dialogue_match(subtitle, dialogue_segments);
            
            if let Some(segment) = best_match {
                // 根據對話片段調整字幕時間
                let adjusted_subtitle = self.adjust_subtitle_timing(subtitle, &segment);
                results.push(SyncResult::Adjusted(adjusted_subtitle));
            } else {
                // 使用原有的同步邏輯
                let correlation_result = self.correlate_subtitle(subtitle).await?;
                results.push(correlation_result);
            }
        }
        
        Ok(results)
    }
    
    fn find_best_dialogue_match(
        &self,
        subtitle: &Subtitle,
        dialogue_segments: &[DialogueSegment],
    ) -> Option<DialogueSegment> {
        let subtitle_duration = subtitle.end_time - subtitle.start_time;
        
        dialogue_segments
            .iter()
            .filter(|segment| segment.is_speech)
            .min_by_key(|segment| {
                // 計算時間和長度的相似度
                let time_diff = (segment.start_time - subtitle.start_time).abs();
                let duration_diff = (segment.duration() - subtitle_duration).abs();
                
                // 組合得分 (越小越好)
                ((time_diff + duration_diff) * 1000.0) as i64
            })
            .cloned()
    }
    
    fn adjust_subtitle_timing(&self, subtitle: &Subtitle, segment: &DialogueSegment) -> Subtitle {
        let mut adjusted = subtitle.clone();
        
        // 根據對話片段調整時間
        let time_offset = segment.start_time - subtitle.start_time;
        adjusted.start_time = segment.start_time;
        adjusted.end_time = subtitle.end_time + time_offset;
        
        // 確保不超出對話片段範圍
        if adjusted.end_time > segment.end_time {
            adjusted.end_time = segment.end_time;
        }
        
        adjusted
    }
}
```

## 配置項目規範

### 統一配置系統整合
對話檢測功能必須完整整合到統一配置系統中：

```toml
# config.toml 新增配置項目
[sync]
dialogue_detection_threshold = 0.01      # 語音檢測閾值
min_dialogue_duration_ms = 500           # 最小對話持續時間（毫秒）
dialogue_merge_gap_ms = 500              # 對話片段合併間隔（毫秒）
enable_dialogue_detection = true         # 是否啟用對話檢測
```

### 配置載入標準
```rust
// 正確的配置載入方式
use crate::config::load_config;

let config = load_config()?;
let threshold = config.sync.dialogue_detection_threshold;
let min_duration = config.sync.min_dialogue_duration_ms;
let enable_detection = config.sync.enable_dialogue_detection;
```

## 測試策略

### 單元測試規範
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    
    #[tokio::test]
    async fn test_energy_analyzer_basic() {
        let analyzer = EnergyAnalyzer::new(0.01, 500);
        let sample_audio = vec![0.0; 44100]; // 1秒靜音
        let segments = analyzer.analyze(&sample_audio, 44100);
        
        // 靜音音訊應該不產生語音片段
        assert!(segments.iter().all(|s| !s.is_speech));
    }
    
    #[tokio::test]
    async fn test_dialogue_segment_operations() {
        let segment1 = DialogueSegment::new_speech(1.0, 3.0);
        let segment2 = DialogueSegment::new_speech(2.5, 4.0);
        
        assert!(segment1.overlaps_with(&segment2));
        assert_eq!(segment1.duration(), 2.0);
    }
    
    #[tokio::test]
    async fn test_dialogue_detector_integration() {
        // 創建測試音訊檔案
        let temp_dir = TempDir::new().unwrap();
        let audio_path = temp_dir.path().join("test.wav");
        
        // 創建模擬音訊檔案（實際測試中需要真實音訊）
        // ...
        
        let detector = DialogueDetector::new().unwrap();
        let segments = detector.detect_dialogue(&audio_path).await.unwrap();
        
        // 驗證檢測結果
        assert!(!segments.is_empty());
    }
    
    #[test]
    fn test_config_loading() {
        // 測試配置載入
        crate::config::init_config_manager().unwrap();
        let config = load_config().unwrap();
        
        assert!(config.sync.dialogue_detection_threshold > 0.0);
        assert!(config.sync.min_dialogue_duration_ms > 0);
    }
}
```

### 整合測試計劃
```rust
// tests/dialogue_detection_integration_tests.rs
use subx_cli::core::sync::dialogue::DialogueDetector;
use subx_cli::core::sync::SyncEngine;

#[tokio::test]
async fn test_sync_engine_with_dialogue_detection() {
    // 建立測試環境
    let temp_dir = tempfile::TempDir::new().unwrap();
    let video_path = temp_dir.path().join("test_video.mp4");
    let subtitle_path = temp_dir.path().join("test_subtitle.srt");
    
    // 創建測試檔案...
    
    // 測試完整的同步流程
    let sync_engine = SyncEngine::new().unwrap();
    let results = sync_engine.sync_with_dialogue_detection(&subtitle_path, &video_path).await;
    
    assert!(results.is_ok());
}
```

## 品質標準

### 程式碼品質檢查
```bash
# 開發過程中必須通過的檢查
cargo fmt
cargo clippy -- -D warnings
cargo test
```

### 錯誤處理標準
```rust
// 在 src/error.rs 中新增對話檢測相關錯誤
impl SubXError {
    pub fn dialogue_detection_failed(msg: String) -> Self {
        Self::AudioProcessing(format!("對話檢測失敗: {}", msg))
    }
    
    pub fn invalid_audio_format(format: String) -> Self {
        Self::AudioProcessing(format!("不支援的音訊格式: {}", format))
    }
    
    pub fn dialogue_segment_invalid(reason: String) -> Self {
        Self::AudioProcessing(format!("無效的對話片段: {}", reason))
    }
}
```

## 驗收標準

### 功能驗收
1. **基本檢測功能**: 能夠識別包含明顯語音的音訊片段
2. **靜音檢測**: 正確識別靜音或背景音樂片段
3. **配置整合**: 所有檢測參數都能透過統一配置系統調整
4. **同步引擎整合**: 檢測結果能有效改善字幕同步準確度

### 技術驗收
1. **模組架構**: 清晰的模組分離和職責劃分
2. **API 一致性**: 與現有核心模組保持一致的 API 設計
3. **錯誤處理**: 完整的錯誤處理和恢復機制
4. **測試覆蓋**: 單元測試覆蓋率達到 80% 以上

### 使用者體驗驗收
1. **透明操作**: 對話檢測過程對使用者透明，不影響現有工作流程
2. **效能表現**: 不顯著增加處理時間
3. **可配置性**: 使用者可以根據需要調整檢測參數
4. **錯誤回報**: 清楚的錯誤訊息和處理建議

## 實作檢查清單

### 開發前準備
- [ ] 確認統一配置系統 (Backlog #14) 已完成並正常運作
- [ ] 檢查現有同步引擎 API 介面
- [ ] 準備不同類型的測試音訊檔案（純語音、音樂混合、噪音等）
- [ ] 確認現有音訊處理服務的相容性

### 階段 1: 基礎架構實作
- [ ] 建立 `src/core/sync/dialogue/` 模組結構
- [ ] 實作 `EnergyAnalyzer` 音訊能量分析器
- [ ] 定義 `DialogueSegment` 和 `SilenceSegment` 資料結構
- [ ] 編寫基礎單元測試

### 階段 2: 檢測器整合
- [ ] 實作 `DialogueDetector` 主檢測器
- [ ] 整合到現有 `SyncEngine`
- [ ] 實作配置載入和參數調整
- [ ] 編寫整合測試

### 階段 3: 最佳化與驗證
- [ ] 實作對話片段最佳化演算法
- [ ] 效能調校和記憶體最佳化
- [ ] 完整測試套件驗證
- [ ] 文件撰寫和範例程式

### 最終驗證
- [ ] 與現有功能完整整合測試
- [ ] 程式碼品質檢查通過
- [ ] 使用者驗收測試通過

## 後續擴展計劃

### 進階檢測功能
- **多語言支援**: 針對不同語言的語音特徵最佳化
- **情感檢測**: 識別語音中的情感特徵
- **說話人分離**: 區分不同說話人的聲音

### 機器學習整合
- **神經網路模型**: 使用深度學習模型提升檢測準確度
- **自適應學習**: 根據使用者資料持續改善檢測效果

### 即時處理支援
- **串流處理**: 支援即時音訊流對話檢測
- **低延遲模式**: 為即時應用最佳化的檢測模式
